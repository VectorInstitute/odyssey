{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:14:45.546088300Z",
     "start_time": "2024-03-13T16:14:43.587090300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "\n",
    "sys.path.append(\"/h/afallah/odyssey/odyssey/lib\")\n",
    "from utils import save_object_to_disk\n",
    "\n",
    "\n",
    "DATA_ROOT = \"/h/afallah/odyssey/odyssey/data/bigbird_data\"\n",
    "DATASET = f\"{DATA_ROOT}/patient_sequences/patient_sequences_2048.parquet\"\n",
    "MAX_LEN = 2048\n",
    "\n",
    "SEED = 23\n",
    "os.chdir(DATA_ROOT)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:12.321718600Z",
     "start_time": "2024-03-13T16:14:45.553089800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: Index(['patient_id', 'num_visits', 'deceased', 'death_after_start',\n",
      "       'death_after_end', 'length', 'token_length', 'event_tokens_2048',\n",
      "       'type_tokens_2048', 'age_tokens_2048', 'time_tokens_2048',\n",
      "       'visit_tokens_2048', 'position_tokens_2048', 'elapsed_tokens_2048',\n",
      "       'common_conditions', 'rare_conditions'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_2048</th>\n",
       "      <th>type_tokens_2048</th>\n",
       "      <th>age_tokens_2048</th>\n",
       "      <th>time_tokens_2048</th>\n",
       "      <th>visit_tokens_2048</th>\n",
       "      <th>position_tokens_2048</th>\n",
       "      <th>elapsed_tokens_2048</th>\n",
       "      <th>common_conditions</th>\n",
       "      <th>rare_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35581927-9c95-5ae9-af76-7d74870a349c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>[[CLS], [VS], 00006473900, 00904516561, 510790...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...</td>\n",
       "      <td>[0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...</td>\n",
       "      <td>[0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f5bba8dd-25c0-5336-8d3d-37424c185026</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>156</td>\n",
       "      <td>[[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...</td>\n",
       "      <td>[0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4938f91-cadb-5133-8541-a52fb0916cea</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>[[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...</td>\n",
       "      <td>[1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...</td>\n",
       "      <td>[0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe2371b-a6f0-5436-aade-7795005b0c66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>[[CLS], [VS], 63739057310, 49281041688, 005970...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...</td>\n",
       "      <td>[0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6f7590ae-f3b9-50e5-9e41-d4bb1000887a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>[[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...</td>\n",
       "      <td>[0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             patient_id  num_visits  deceased  \\\n",
       "0  35581927-9c95-5ae9-af76-7d74870a349c           1         0   \n",
       "1  f5bba8dd-25c0-5336-8d3d-37424c185026           2         0   \n",
       "2  f4938f91-cadb-5133-8541-a52fb0916cea           2         0   \n",
       "3  6fe2371b-a6f0-5436-aade-7795005b0c66           2         0   \n",
       "4  6f7590ae-f3b9-50e5-9e41-d4bb1000887a           1         0   \n",
       "\n",
       "   death_after_start  death_after_end  length  token_length  \\\n",
       "0                NaN              NaN      50            54   \n",
       "1                NaN              NaN     148           156   \n",
       "2                NaN              NaN      78            86   \n",
       "3                NaN              NaN      86            94   \n",
       "4                NaN              NaN      72            76   \n",
       "\n",
       "                                   event_tokens_2048  \\\n",
       "0  [[CLS], [VS], 00006473900, 00904516561, 510790...   \n",
       "1  [[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...   \n",
       "2  [[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...   \n",
       "3  [[CLS], [VS], 63739057310, 49281041688, 005970...   \n",
       "4  [[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...   \n",
       "\n",
       "                                    type_tokens_2048  \\\n",
       "0  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...   \n",
       "1  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2  [1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "3  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "\n",
       "                                     age_tokens_2048  \\\n",
       "0  [0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...   \n",
       "1  [0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...   \n",
       "2  [0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...   \n",
       "3  [0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...   \n",
       "4  [0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...   \n",
       "\n",
       "                                    time_tokens_2048  \\\n",
       "0  [0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...   \n",
       "1  [0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...   \n",
       "2  [0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...   \n",
       "3  [0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...   \n",
       "4  [0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...   \n",
       "\n",
       "                                   visit_tokens_2048  \\\n",
       "0  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                position_tokens_2048  \\\n",
       "0  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                 elapsed_tokens_2048  \\\n",
       "0  [-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...   \n",
       "1  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  [-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...   \n",
       "3  [-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n",
       "4  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                common_conditions                 rare_conditions  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "print(f\"Current columns: {dataset_2048.columns}\")\n",
    "dataset_2048.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_num_visit(dataset: pd.DataFrame, minimum_num_visits: int) -> pd.DataFrame:\n",
    "    \"\"\"Filter the patients based on num_visits threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold num_visits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset[\"num_visits\"] >= minimum_num_visits]\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def filter_by_length_of_stay(dataset: pd.DataFrame, threshold: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Filter the patients based on length of stay threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold length of stay\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset[\"length_of_stay\"] >= threshold]\n",
    "\n",
    "    # Only keep the patients that their first event happens within threshold\n",
    "    # TODO: Check how many patients get removed here?\n",
    "    filtered_dataset = filtered_dataset[\n",
    "        filtered_dataset.apply(\n",
    "            lambda row: row[\"elapsed_tokens_2048\"][row[\"last_VS_index\"] + 1]\n",
    "            < threshold * 24,\n",
    "            axis=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def get_last_occurence_index(seq: List[str], target: str) -> int:\n",
    "    \"\"\"Return the index of the last occurrence of target in seq.\n",
    "\n",
    "    Args:\n",
    "        seq (List[str]): The input sequence.\n",
    "        target (str): The target string to find.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        int: The index of the last occurrence of target in seq.\n",
    "    \"\"\"\n",
    "    return len(seq) - (seq[::-1].index(target) + 1)\n",
    "\n",
    "\n",
    "def check_readmission_label(row: pd.Series) -> int:\n",
    "    \"\"\"Check if the label indicates readmission within one month.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bool: True if readmission label is present, False otherwise.\n",
    "    \"\"\"\n",
    "    last_vs_index = row[\"last_VS_index\"]\n",
    "    return int(\n",
    "        row[\"event_tokens_2048\"][last_vs_index - 1]\n",
    "        in (\"[W_0]\", \"[W_1]\", \"[W_2]\", \"[W_3]\", \"[M_1]\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_length_of_stay(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"Determine the length of a given visit.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.Series: The preprocessed row.\n",
    "    \"\"\"\n",
    "    admission_time = row[\"last_VS_index\"] + 1\n",
    "    discharge_time = row[\"last_VE_index\"] - 1\n",
    "    return (discharge_time - admission_time) / 24\n",
    "\n",
    "\n",
    "def get_visit_cutoff_at_threshold(row: pd.Series, threshold: int = 24) -> int:\n",
    "    \"\"\"Get the index of the first event token of last visit that occurs after threshold hours.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "        threshold (int): The number of hours to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        cutoff_index (int): The corrosponding cutoff index.\n",
    "    \"\"\"\n",
    "    last_vs_index = row[\"last_VS_index\"]\n",
    "    last_ve_index = row[\"last_VE_index\"]\n",
    "\n",
    "    for i in range(last_vs_index + 1, last_ve_index):\n",
    "        if row[\"elapsed_tokens_2048\"][i] > threshold:\n",
    "            return i\n",
    "\n",
    "    return len(row[\"event_tokens_2048\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_length_of_stay_dataset(\n",
    "    dataset: pd.DataFrame,\n",
    "    threshold: int = 7,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process the length of stay dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        threshold (int): The threshold length of stay.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset[\"last_VS_index\"] = dataset[\"event_tokens_2048\"].transform(\n",
    "        lambda seq: get_last_occurence_index(list(seq), \"[VS]\"),\n",
    "    )\n",
    "    dataset[\"last_VE_index\"] = dataset[\"event_tokens_2048\"].transform(\n",
    "        lambda seq: get_last_occurence_index(list(seq), \"[VE]\"),\n",
    "    )\n",
    "    dataset[\"length_of_stay\"] = dataset.apply(get_length_of_stay, axis=1)\n",
    "\n",
    "    dataset = filter_by_length_of_stay(dataset, threshold=1)\n",
    "    dataset[\"label_los_1week\"] = (dataset[\"length_of_stay\"] >= threshold).astype(int)\n",
    "\n",
    "    dataset[\"cutoff_los\"] = dataset.apply(\n",
    "        lambda row: get_visit_cutoff_at_threshold(row, threshold=24),\n",
    "        axis=1,\n",
    "    )\n",
    "    dataset[\"token_length\"] = dataset[\"event_tokens_2048\"].apply(len)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Process the dataset for length of stay prediction above a threshold\n",
    "dataset_2048_los = process_length_of_stay_dataset(dataset_2048.copy(), threshold=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_condition_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the condition dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input condition dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The processed condition dataset.\n",
    "    \"\"\"\n",
    "    dataset[\"all_conditions\"] = dataset.apply(\n",
    "        lambda row: np.concatenate(\n",
    "            [row[\"common_conditions\"], row[\"rare_conditions\"]],\n",
    "            dtype=np.int64,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Process the dataset for conditions including rare and common\n",
    "dataset_2048_condition = process_condition_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:16.075719400Z",
     "start_time": "2024-03-13T16:15:12.335721100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mortality_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the mortality dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input mortality dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The processed mortality dataset.\n",
    "    \"\"\"\n",
    "    dataset[\"label_mortality_2weeks\"] = (\n",
    "        (dataset[\"death_after_start\"] >= 0) & (dataset[\"death_after_end\"] <= 15)\n",
    "    ).astype(int)\n",
    "    dataset[\"label_mortality_1month\"] = (\n",
    "        (dataset[\"death_after_start\"] >= 0) & (dataset[\"death_after_end\"] <= 32)\n",
    "    ).astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:47.326996100Z",
     "start_time": "2024-03-13T16:15:16.094719300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_readmission_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the readmission dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset[\"last_VS_index\"] = dataset[\"event_tokens_2048\"].transform(\n",
    "        lambda seq: get_last_occurence_index(list(seq), \"[VS]\"),\n",
    "    )\n",
    "    dataset[\"cutoff_readmission\"] = dataset[\"last_VS_index\"] - 1\n",
    "    dataset[\"label_readmission_1month\"] = dataset.apply(check_readmission_label, axis=1)\n",
    "\n",
    "    dataset[\"num_visits\"] -= 1\n",
    "    dataset[\"token_length\"] = dataset[\"event_tokens_2048\"].apply(len)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = filter_by_num_visit(\n",
    "    dataset_2048.copy(),\n",
    "    minimum_num_visits=2,\n",
    ")\n",
    "dataset_2048_readmission = process_readmission_dataset(dataset_2048_readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_dataset(datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Process the multi-task dataset by merging the original dataset with the other datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (Dict): Dictionary mapping each task to its respective dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pd.DataFrame: The processed multi-task dataset\n",
    "    \"\"\"\n",
    "    # Merging datasets on 'patient_id'\n",
    "    multi_dataset = datasets[\"original\"].merge(\n",
    "        datasets[\"condition\"][[\"patient_id\", \"all_conditions\"]],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    multi_dataset = multi_dataset.merge(\n",
    "        datasets[\"mortality\"][[\"patient_id\", \"label_mortality_1month\"]],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    multi_dataset = multi_dataset.merge(\n",
    "        datasets[\"readmission\"][\n",
    "            [\"patient_id\", \"cutoff_readmission\", \"label_readmission_1month\"]\n",
    "        ],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    multi_dataset = multi_dataset.merge(\n",
    "        datasets[\"los\"][[\"patient_id\", \"cutoff_los\", \"label_los_1week\"]],\n",
    "        on=\"patient_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Selecting the required columns\n",
    "    multi_dataset = multi_dataset[\n",
    "        [\n",
    "            \"patient_id\",\n",
    "            \"num_visits\",\n",
    "            \"event_tokens_2048\",\n",
    "            \"type_tokens_2048\",\n",
    "            \"age_tokens_2048\",\n",
    "            \"time_tokens_2048\",\n",
    "            \"visit_tokens_2048\",\n",
    "            \"position_tokens_2048\",\n",
    "            \"elapsed_tokens_2048\",\n",
    "            \"cutoff_los\",\n",
    "            \"cutoff_readmission\",\n",
    "            \"all_conditions\",\n",
    "            \"label_mortality_1month\",\n",
    "            \"label_readmission_1month\",\n",
    "            \"label_los_1week\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Transform conditions from a vector of numbers to binary classes\n",
    "    conditions_expanded = multi_dataset[\"all_conditions\"].apply(pd.Series)\n",
    "    conditions_expanded.columns = [f\"condition{i}\" for i in range(20)]\n",
    "    multi_dataset = multi_dataset.drop(\"all_conditions\", axis=1)\n",
    "    multi_dataset = pd.concat([multi_dataset, conditions_expanded], axis=1)\n",
    "\n",
    "    # Standardize important column names\n",
    "    multi_dataset.rename(\n",
    "        columns={\n",
    "            \"cutoff_los\": \"cutoff_los_1week\",\n",
    "            \"cutoff_readmission\": \"cutoff_readmission_1month\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    condition_columns = {f\"condition{i}\": f\"label_c{i}\" for i in range(20)}\n",
    "    multi_dataset.rename(columns=condition_columns, inplace=True)\n",
    "\n",
    "    numerical_columns = [\n",
    "        \"cutoff_los_1week\",\n",
    "        \"cutoff_readmission_1month\",\n",
    "        \"label_mortality_1month\",\n",
    "        \"label_readmission_1month\",\n",
    "        \"label_los_1week\",\n",
    "    ] + [f\"label_c{i}\" for i in range(20)]\n",
    "\n",
    "    # Fill NaN values and convert to integers\n",
    "    for column in numerical_columns:\n",
    "        multi_dataset[column] = multi_dataset[column].fillna(-1).astype(int)\n",
    "\n",
    "    # Reset dataset index\n",
    "    multi_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return multi_dataset\n",
    "\n",
    "\n",
    "multi_dataset = process_multi_dataset(\n",
    "    datasets={\n",
    "        \"original\": dataset_2048,\n",
    "        \"mortality\": dataset_2048_mortality,\n",
    "        \"condition\": dataset_2048_condition,\n",
    "        \"readmission\": dataset_2048_readmission,\n",
    "        \"los\": dataset_2048_los,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(\n",
    "    dataset: pd.DataFrame,\n",
    "    target: str,\n",
    "    test_size: float,\n",
    "    return_test: Optional[bool] = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split the given dataset into training and testing sets using iterative stratification on given multi-label target.\n",
    "    \"\"\"\n",
    "    # Convert all_conditions into a format suitable for multi-label stratification\n",
    "    Y = np.array(dataset[target].values.tolist())\n",
    "    X = dataset[\"patient_id\"].to_numpy().reshape(-1, 1)\n",
    "    is_single_label = type(dataset.iloc[0][target]) == np.int64\n",
    "\n",
    "    # Perform stratified split\n",
    "    if is_single_label:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            Y,\n",
    "            stratify=Y,\n",
    "            test_size=test_size,\n",
    "            random_state=SEED,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = iterative_train_test_split(\n",
    "            X,\n",
    "            Y,\n",
    "            test_size=test_size,\n",
    "        )\n",
    "\n",
    "    X_train = X_train.flatten().tolist()\n",
    "    X_test = X_test.flatten().tolist()\n",
    "\n",
    "    if return_test:\n",
    "        return X_test\n",
    "    else:\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "def sample_balanced_subset(dataset: pd.DataFrame, target: str, sample_size: int):\n",
    "    \"\"\"\n",
    "    Sample a subset of dataset with balanced target labels.\n",
    "    \"\"\"\n",
    "    # Sampling positive and negative patients\n",
    "    pos_patients = dataset[dataset[target] == True].sample(\n",
    "        n=sample_size // 2,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    neg_patients = dataset[dataset[target] == False].sample(\n",
    "        n=sample_size // 2,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "\n",
    "    # Combining and shuffling patient IDs\n",
    "    sample_patients = (\n",
    "        pos_patients[\"patient_id\"].tolist() + neg_patients[\"patient_id\"].tolist()\n",
    "    )\n",
    "    random.shuffle(sample_patients)\n",
    "\n",
    "    return sample_patients\n",
    "\n",
    "\n",
    "def get_pretrain_test_split(\n",
    "    dataset: pd.DataFrame,\n",
    "    stratify_target: Optional[str] = None,\n",
    "    test_size: float = 0.15,\n",
    "):\n",
    "    \"\"\"Split dataset into pretrain and test set. Stratify on a given target column if needed.\"\"\"\n",
    "    if stratify_target:\n",
    "        pretrain_ids, test_ids = stratified_train_test_split(\n",
    "            dataset,\n",
    "            target=stratify_target,\n",
    "            test_size=test_size,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=SEED)\n",
    "        test_ids = test_patients[\"patient_id\"].tolist()\n",
    "        pretrain_ids = dataset[~dataset[\"patient_id\"].isin(test_patients)][\n",
    "            \"patient_id\"\n",
    "        ].tolist()\n",
    "\n",
    "    random.shuffle(pretrain_ids)\n",
    "\n",
    "    return pretrain_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "patient_ids_dict = {\n",
    "    \"pretrain\": [],\n",
    "    \"finetune\": {\"few_shot\": {}, \"kfold\": {}},\n",
    "    \"test\": [],\n",
    "}\n",
    "\n",
    "# Get train-test split\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(dataset_2048_readmission, stratify_target='label_readmission_1month', test_size=0.2)\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(process_condition_dataset, stratify_target='all_conditions', test_size=0.15)\n",
    "# patient_ids_dict['pretrain'] = pretrain_ids\n",
    "# patient_ids_dict['test'] = test_ids\n",
    "\n",
    "# Load pretrain and test patient IDs\n",
    "pid = pickle.load(open(\"patient_id_dict/dataset_2048_multi.pkl\", \"rb\"))\n",
    "patient_ids_dict[\"pretrain\"] = pid[\"pretrain\"]\n",
    "patient_ids_dict[\"test\"] = pid[\"test\"]\n",
    "set(pid[\"pretrain\"] + pid[\"test\"]) == set(dataset_2048[\"patient_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    task_splits = {\n",
    "        \"mortality\": {\n",
    "            \"dataset\": dataset_2048_mortality,\n",
    "            \"label_col\": \"label_mortality_1month\",\n",
    "            \"finetune_size\": [250, 500, 1000, 5000, 20000],\n",
    "            \"save_path\": \"patient_id_dict/dataset_2048_mortality.pkl\",\n",
    "            \"split_mode\": \"single_label_balanced\",\n",
    "        },\n",
    "        \"readmission\": {\n",
    "            \"dataset\": dataset_2048_readmission,\n",
    "            \"label_col\": \"label_readmission_1month\",\n",
    "            \"finetune_size\": [250, 1000, 5000, 20000, 60000],\n",
    "            \"save_path\": \"patient_id_dict/dataset_2048_readmission.pkl\",\n",
    "            \"split_mode\": \"single_label_stratified\",\n",
    "        },\n",
    "        \"length_of_stay\": {\n",
    "            \"dataset\": dataset_2048_los,\n",
    "            \"label_col\": \"label_los_1week\",\n",
    "            \"finetune_size\": [250, 1000, 5000, 20000, 50000],\n",
    "            \"save_path\": \"patient_id_dict/dataset_2048_los.pkl\",\n",
    "            \"split_mode\": \"single_label_balanced\",\n",
    "        },\n",
    "        \"condition\": {\n",
    "            \"dataset\": dataset_2048_condition,\n",
    "            \"label_col\": \"all_conditions\",\n",
    "            \"finetune_size\": [50000],\n",
    "            \"save_path\": \"patient_id_dict/dataset_2048_condition.pkl\",\n",
    "            \"split_mode\": \"multi_label_stratified\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    all_tasks = list(task_splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:51.800996200Z",
     "start_time": "2024-03-13T16:15:50.494996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_finetune_split(\n",
    "    config: config,\n",
    "    patient_ids_dict: Dict[str, Any],\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-finetuneation sets using k-fold cross-finetuneation\n",
    "    while ensuring balanced label distribution in each fold. Saves the resulting dictionary to disk.\n",
    "    \"\"\"\n",
    "    # Extract task-specific configuration\n",
    "    task_config = config.task_splits[task]\n",
    "    dataset = task_config[\"dataset\"]\n",
    "    label_col = task_config[\"label_col\"]\n",
    "    finetune_sizes = task_config[\"finetune_size\"]\n",
    "    save_path = task_config[\"save_path\"]\n",
    "    split_mode = task_config[\"split_mode\"]\n",
    "\n",
    "    # Get pretrain dataset\n",
    "    pretrain_ids = patient_ids_dict[\"pretrain\"]\n",
    "    dataset = dataset[dataset[\"patient_id\"].isin(pretrain_ids)]\n",
    "\n",
    "    # Few-shot finetune patient ids\n",
    "    for finetune_num in finetune_sizes:\n",
    "        if split_mode == \"single_label_balanced\":\n",
    "            finetune_ids = sample_balanced_subset(\n",
    "                dataset,\n",
    "                target=label_col,\n",
    "                sample_size=finetune_num,\n",
    "            )\n",
    "\n",
    "        elif (\n",
    "            split_mode == \"single_label_stratified\"\n",
    "            or split_mode == \"multi_label_stratified\"\n",
    "        ):\n",
    "            finetune_ids = stratified_train_test_split(\n",
    "                dataset,\n",
    "                target=label_col,\n",
    "                test_size=finetune_num / len(dataset),\n",
    "                return_test=True,\n",
    "            )\n",
    "\n",
    "        patient_ids_dict[\"finetune\"][\"few_shot\"][f\"{finetune_num}\"] = finetune_ids\n",
    "\n",
    "    # Save the dictionary to disk\n",
    "    save_object_to_disk(patient_ids_dict, save_path)\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "\n",
    "for task in config.all_tasks:\n",
    "    patient_ids_dict = get_finetune_split(\n",
    "        config=config,\n",
    "        patient_ids_dict=patient_ids_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:14:10.181184300Z",
     "start_time": "2024-03-13T14:13:39.154567400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_2048_mortality.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_mortality.parquet\",\n",
    ")\n",
    "dataset_2048_readmission.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_readmission.parquet\",\n",
    ")\n",
    "dataset_2048_los.to_parquet(\"patient_sequences/patient_sequences_2048_los.parquet\")\n",
    "dataset_2048_condition.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_condition.parquet\",\n",
    ")\n",
    "multi_dataset.to_parquet(\"patient_sequences/patient_sequences_2048_multi.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# multi_dataset = pd.read_parquet('patient_sequences/patient_sequences_2048_multi.parquet')\n",
    "# pid = pickle.load(open('patient_id_dict/dataset_2048_multi.pkl', 'rb'))\n",
    "# multi_dataset = multi_dataset[multi_dataset['patient_id'].isin(pid['finetune']['few_shot']['all'])]\n",
    "\n",
    "# # Train Tokenizer\n",
    "# tokenizer = ConceptTokenizer(data_dir='/h/afallah/odyssey/odyssey/data/vocab')\n",
    "# tokenizer.fit_on_vocab()\n",
    "\n",
    "# # Load datasets\n",
    "# tasks = ['mortality_1month', 'los_1week'] + [f'c{i}' for i in range(5)]\n",
    "\n",
    "# train_dataset = FinetuneMultiDataset(\n",
    "#     data=multi_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     tasks=tasks,\n",
    "#     balance_guide={'mortality_1month': 0.5, 'los_1week': 0.5},\n",
    "#     max_len=2048,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2048_condition = pd.read_parquet('patient_sequences/patient_sequences_2048_condition.parquet')\n",
    "# pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "# condition_finetune = dataset_2048_condition.loc[dataset_2048_condition['patient_id'].isin(pid['finetune']['few_shot']['50000'])]\n",
    "# condition_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = np.array(condition_finetune['all_conditions'].tolist()).sum(axis=0)\n",
    "# weights = np.clip(0, 50, sum(freq) / freq)\n",
    "# np.max(np.sqrt(freq)) / np.sqrt(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(patient_ids_dict['pretrain']) == sorted(pickle.load(open('new_data/patient_id_dict/sample_pretrain_test_patient_ids_with_conditions.pkl', 'rb'))['pretrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(dataset_2048_mortality, dataset_2048_readmission, how='outer', on='patient_id')\n",
    "# final_merged_df = pd.merge(merged_df, dataset_2048_condition, how='outer', on='patient_id')\n",
    "# final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing stratified k-fold split\n",
    "# skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "#     dataset_cv = dataset.iloc[cv_index]\n",
    "#     dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "#     # Separate positive and negative labeled patients\n",
    "#     pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "#     neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "#     # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "#     num_pos_needed = cv_size // 2\n",
    "#     num_neg_needed = cv_size // 2\n",
    "\n",
    "#     # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "#     cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "#     remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "#     # Extract patient IDs for training set\n",
    "#     finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "#     finetune_patients += remaining_finetune_patients\n",
    "\n",
    "#     # Shuffle each list of patients\n",
    "#     random.shuffle(cv_patients)\n",
    "#     random.shuffle(finetune_patients)\n",
    "\n",
    "#     patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "# dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "# plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "# plt.ylim(0, 6000)\n",
    "# plt.xlabel('Length of Event Tokens')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Event Tokens Length')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_occurence_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
