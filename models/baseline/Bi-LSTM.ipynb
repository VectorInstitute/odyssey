{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:51:38.262324100Z",
     "start_time": "2024-01-29T14:51:38.161326900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: Bi-LSTM.ipynb\n",
    "Code to train and evaluate a bi-directional LSTM model on MIMIC-IV FHIR dataset.\n",
    "\"\"\"\n",
    "\n",
    "def Project():\n",
    "    \"\"\"\n",
    "    __Objectives__\n",
    "    0. Import data and tokenizer\n",
    "    1. Train the tokenizer on all sequences of the dataset\n",
    "    2. Tokenize different sequences and join them together\n",
    "    3. Prepare actual labels for one, six, twelve month death after discharge\n",
    "    4. Define the model architecture for bidrectional LSTM\n",
    "    5. Train Bi-LSTM model and evaluate on test dataset\n",
    "    >>> 6. Compare performance across new tasks to XGBoost\n",
    "    \"\"\"\n",
    "    return ProjectObjectives.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:12.085345400Z",
     "start_time": "2024-01-29T14:51:38.200323200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 09:51:52.773009: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 09:51:53.278475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 09:51:53.278634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 09:51:53.313311: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 09:51:53.447516: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 09:51:53.450588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 09:51:58.317937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os; ROOT = '/fs01/home/afallah/odyssey/slurm'; os.chdir(ROOT)\n",
    "import sys\n",
    "import scipy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack, save_npz, load_npz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import relu, leaky_relu, sigmoid\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from models.cehr_bert.data import PretrainDataset, FinetuneDataset\n",
    "from models.cehr_bert.model import BertPretrain\n",
    "from models.cehr_bert.tokenizer import ConceptTokenizer\n",
    "from models.cehr_bert.embeddings import Embeddings\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_ROOT = f'{ROOT}/data'\n",
    "DATA_PATH = f'{DATA_ROOT}/patient_sequences.parquet'\n",
    "SAMPLE_DATA_PATH = f'{DATA_ROOT}/CEHR-BERT_sample_patient_sequence.parquet'\n",
    "FREQ_DF_PATH = f'{DATA_ROOT}/patient_feature_freq.csv'\n",
    "FREQ_MATRIX_PATH = f'{DATA_ROOT}/patient_freq_matrix.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:12.259348300Z",
     "start_time": "2024-01-29T14:52:12.094345300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# save parameters and configurations\n",
    "class config:\n",
    "    seed = 23\n",
    "    data_dir = DATA_ROOT\n",
    "    test_size = 0.2\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    vocab_size = None\n",
    "    embedding_size = 128\n",
    "    time_embeddings_size = 16\n",
    "    max_len = 512\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # pl.seed_everything(seed)\n",
    "\n",
    "\n",
    "seed_all(config.seed)\n",
    "print(f'Cuda: {torch.cuda.get_device_name(torch.cuda.current_device())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:26.546344100Z",
     "start_time": "2024-01-29T14:52:12.258344100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>label</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>new_start</th>\n",
       "      <th>event_tokens_untruncated</th>\n",
       "      <th>event_tokens</th>\n",
       "      <th>age_tokens</th>\n",
       "      <th>time_tokens</th>\n",
       "      <th>visit_tokens</th>\n",
       "      <th>position_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be7990af-3829-5df0-b552-c397a71d46fe</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 4443, 00338004304, 00006473900, 000935211...</td>\n",
       "      <td>[VS, 4443, 00338004304, 00006473900, 000935211...</td>\n",
       "      <td>[66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 6...</td>\n",
       "      <td>[8000, 8000, 8000, 8000, 8000, 8000, 8000, 800...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>877d281b-676b-53ab-9911-1e4677989f6f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 741, 00182864389, 00904585461, 0070345020...</td>\n",
       "      <td>[VS, 741, 00182864389, 00904585461, 0070345020...</td>\n",
       "      <td>[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 3...</td>\n",
       "      <td>[5085, 5085, 5085, 5085, 5085, 5085, 5085, 508...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65ae1ba2-dede-53a4-80be-3d0666b27e87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 51248_2, 51736_2, 51244_3, 51222_4, 51737...</td>\n",
       "      <td>[VS, 51248_2, 51736_2, 51244_3, 51222_4, 51737...</td>\n",
       "      <td>[24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2...</td>\n",
       "      <td>[8787, 8787, 8787, 8787, 8787, 8787, 8787, 878...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa1446f6-dbc4-5734-9645-a1e01a7ba6f0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 0689, 33332000801, 00056017075, 655970103...</td>\n",
       "      <td>[VS, 0689, 33332000801, 00056017075, 655970103...</td>\n",
       "      <td>[77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 7...</td>\n",
       "      <td>[4853, 4853, 4853, 4853, 4853, 4853, 4853, 485...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b3c303cc-df8c-5789-80f0-83f1c319b813</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 7935, 00338067104, 00054855324, 009045165...</td>\n",
       "      <td>[VS, 7935, 00338067104, 00054855324, 009045165...</td>\n",
       "      <td>[62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 6...</td>\n",
       "      <td>[6037, 6037, 6037, 6037, 6037, 6037, 6037, 603...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90273</th>\n",
       "      <td>88ae054e-0173-5049-b067-a67bad1aeee9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 7931, 00075062041, 00023050601, 005363381...</td>\n",
       "      <td>[VS, 7931, 00075062041, 00023050601, 005363381...</td>\n",
       "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
       "      <td>[8788, 8788, 8788, 8788, 8788, 8788, 8788, 878...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90274</th>\n",
       "      <td>3b6ec88d-59a8-5833-8977-48e8b58211b1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 00338067104, 51079045620, 66553000401, 00...</td>\n",
       "      <td>[VS, 00338067104, 51079045620, 66553000401, 00...</td>\n",
       "      <td>[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...</td>\n",
       "      <td>[5353, 5353, 5353, 5353, 5353, 5353, 5353, 535...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90275</th>\n",
       "      <td>b883470b-664e-5f0e-b38c-717cd5b07b84</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 5503, 00338004904, 00006494300, 001828447...</td>\n",
       "      <td>[VS, 5503, 00338004904, 00006494300, 001828447...</td>\n",
       "      <td>[81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 8...</td>\n",
       "      <td>[7450, 7450, 7450, 7450, 7450, 7450, 7450, 745...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90277</th>\n",
       "      <td>c946654b-2765-5dc1-8cd4-9865d3c84d30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 51079088120, 51079088120, 68084025401, 00...</td>\n",
       "      <td>[VS, 51079088120, 51079088120, 68084025401, 00...</td>\n",
       "      <td>[45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 4...</td>\n",
       "      <td>[4850, 4850, 4850, 4850, 4850, 4850, 4850, 485...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90278</th>\n",
       "      <td>fd4c2513-8fb6-56f3-b142-009e3d0f520f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[VS, 8151, 0077, 00574705050, 00904516561, 003...</td>\n",
       "      <td>[VS, 8151, 0077, 00574705050, 00904516561, 003...</td>\n",
       "      <td>[68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...</td>\n",
       "      <td>[8480, 8480, 8480, 8480, 8480, 8480, 8480, 848...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173671 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 patient_id  num_visits  label  \\\n",
       "0      be7990af-3829-5df0-b552-c397a71d46fe           3      0   \n",
       "1      877d281b-676b-53ab-9911-1e4677989f6f           1      0   \n",
       "2      65ae1ba2-dede-53a4-80be-3d0666b27e87           1      0   \n",
       "3      aa1446f6-dbc4-5734-9645-a1e01a7ba6f0           1      0   \n",
       "4      b3c303cc-df8c-5789-80f0-83f1c319b813           1      1   \n",
       "...                                     ...         ...    ...   \n",
       "90273  88ae054e-0173-5049-b067-a67bad1aeee9           1      0   \n",
       "90274  3b6ec88d-59a8-5833-8977-48e8b58211b1           1      0   \n",
       "90275  b883470b-664e-5f0e-b38c-717cd5b07b84           1      1   \n",
       "90277  c946654b-2765-5dc1-8cd4-9865d3c84d30           2      0   \n",
       "90278  fd4c2513-8fb6-56f3-b142-009e3d0f520f           1      0   \n",
       "\n",
       "       death_after_start  death_after_end  length  token_length  new_start  \\\n",
       "0                    NaN              NaN     217           225        NaN   \n",
       "1                    NaN              NaN      18            20        NaN   \n",
       "2                    NaN              NaN      40            42        NaN   \n",
       "3                    NaN              NaN      18            20        NaN   \n",
       "4                   22.0             17.0      81            83        NaN   \n",
       "...                  ...              ...     ...           ...        ...   \n",
       "90273                NaN              NaN      36            38        NaN   \n",
       "90274                NaN              NaN      17            19        NaN   \n",
       "90275                3.0              0.0     152           154        NaN   \n",
       "90277                NaN              NaN      46            51        NaN   \n",
       "90278                NaN              NaN      76            78        NaN   \n",
       "\n",
       "                                event_tokens_untruncated  \\\n",
       "0      [VS, 4443, 00338004304, 00006473900, 000935211...   \n",
       "1      [VS, 741, 00182864389, 00904585461, 0070345020...   \n",
       "2      [VS, 51248_2, 51736_2, 51244_3, 51222_4, 51737...   \n",
       "3      [VS, 0689, 33332000801, 00056017075, 655970103...   \n",
       "4      [VS, 7935, 00338067104, 00054855324, 009045165...   \n",
       "...                                                  ...   \n",
       "90273  [VS, 7931, 00075062041, 00023050601, 005363381...   \n",
       "90274  [VS, 00338067104, 51079045620, 66553000401, 00...   \n",
       "90275  [VS, 5503, 00338004904, 00006494300, 001828447...   \n",
       "90277  [VS, 51079088120, 51079088120, 68084025401, 00...   \n",
       "90278  [VS, 8151, 0077, 00574705050, 00904516561, 003...   \n",
       "\n",
       "                                            event_tokens  \\\n",
       "0      [VS, 4443, 00338004304, 00006473900, 000935211...   \n",
       "1      [VS, 741, 00182864389, 00904585461, 0070345020...   \n",
       "2      [VS, 51248_2, 51736_2, 51244_3, 51222_4, 51737...   \n",
       "3      [VS, 0689, 33332000801, 00056017075, 655970103...   \n",
       "4      [VS, 7935, 00338067104, 00054855324, 009045165...   \n",
       "...                                                  ...   \n",
       "90273  [VS, 7931, 00075062041, 00023050601, 005363381...   \n",
       "90274  [VS, 00338067104, 51079045620, 66553000401, 00...   \n",
       "90275  [VS, 5503, 00338004904, 00006494300, 001828447...   \n",
       "90277  [VS, 51079088120, 51079088120, 68084025401, 00...   \n",
       "90278  [VS, 8151, 0077, 00574705050, 00904516561, 003...   \n",
       "\n",
       "                                              age_tokens  \\\n",
       "0      [66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 6...   \n",
       "1      [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 3...   \n",
       "2      [24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2...   \n",
       "3      [77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 7...   \n",
       "4      [62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 6...   \n",
       "...                                                  ...   \n",
       "90273  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
       "90274  [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...   \n",
       "90275  [81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 8...   \n",
       "90277  [45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 4...   \n",
       "90278  [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...   \n",
       "\n",
       "                                             time_tokens  \\\n",
       "0      [8000, 8000, 8000, 8000, 8000, 8000, 8000, 800...   \n",
       "1      [5085, 5085, 5085, 5085, 5085, 5085, 5085, 508...   \n",
       "2      [8787, 8787, 8787, 8787, 8787, 8787, 8787, 878...   \n",
       "3      [4853, 4853, 4853, 4853, 4853, 4853, 4853, 485...   \n",
       "4      [6037, 6037, 6037, 6037, 6037, 6037, 6037, 603...   \n",
       "...                                                  ...   \n",
       "90273  [8788, 8788, 8788, 8788, 8788, 8788, 8788, 878...   \n",
       "90274  [5353, 5353, 5353, 5353, 5353, 5353, 5353, 535...   \n",
       "90275  [7450, 7450, 7450, 7450, 7450, 7450, 7450, 745...   \n",
       "90277  [4850, 4850, 4850, 4850, 4850, 4850, 4850, 485...   \n",
       "90278  [8480, 8480, 8480, 8480, 8480, 8480, 8480, 848...   \n",
       "\n",
       "                                            visit_tokens  \\\n",
       "0      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                  ...   \n",
       "90273  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "90274  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "90275  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "90277  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "90278  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                         position_tokens  \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "90273  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "90274  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "90275  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "90277  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "90278  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[173671 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_parquet(DATA_PATH)\n",
    "data.rename(columns={'event_tokens': 'event_tokens_untruncated', 'event_tokens_updated': 'event_tokens'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:26.547343500Z",
     "start_time": "2024-01-29T14:52:26.537347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define custom labels, here death in 12 M\n",
    "data['label'] = ((data['death_after_end'] > 0) & (data['death_after_end'] < 365)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:52:26.990344200Z",
     "start_time": "2024-01-29T14:52:26.537347Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.cehr_bert.tokenizer.ConceptTokenizer at 0x7ffa91a9c7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit tokenizer on .json vocab files\n",
    "tokenizer = ConceptTokenizer(data_dir=config.data_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "config.vocab_size = tokenizer.get_vocab_size()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define dataset with token lengths\n",
    "class DatasetWithTokenLength(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_data, length_data):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.length_data = length_data\n",
    "        assert len(tokenized_data) == len(length_data), \"Datasets have different lengths\"\n",
    "\n",
    "        self.sorted_indices = sorted(range(len(length_data)), key=lambda x: length_data[x], reverse=True)\n",
    "        # self.tokenized_data = [tokenized_data[i] for i in self.sorted_indices]\n",
    "        # self.length_data = [min(length_data[i], ) for i in self.sorted_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.sorted_indices[index]\n",
    "        return self.tokenized_data[index], min(config.max_len, self.length_data[index])\n",
    "\n",
    "\n",
    "# Get training and test datasets\n",
    "train_data, test_data = train_test_split(\n",
    "    data,\n",
    "    test_size=config.test_size,\n",
    "    random_state=config.seed,\n",
    "    stratify=data['label']\n",
    ")\n",
    "\n",
    "train_dataset = FinetuneDataset(\n",
    "    data=train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len,\n",
    ")\n",
    "\n",
    "test_dataset = FinetuneDataset(\n",
    "    data=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len,\n",
    ")\n",
    "\n",
    "train_dataset_with_lengths = DatasetWithTokenLength(train_dataset, train_data['token_length'].values)\n",
    "test_dataset_with_lengths = DatasetWithTokenLength(test_dataset, test_data['token_length'].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_with_lengths,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_with_lengths,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Data is ready to go!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class BiLSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.embedding_size,\n",
    "            time_embedding_size=config.time_embeddings_size,\n",
    "            max_len=config.max_len)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout_rate)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        embed = self.embeddings(*inputs)\n",
    "        packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(packed_embed)\n",
    "        # output = lstm_out[:, -1, :]\n",
    "        output = torch.cat((hidden_state[-2, :, :], hidden_state[-1, :, :]), dim=1)\n",
    "\n",
    "        output = self.dropout(self.batch_norm(output))\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_inputs_labels(sequences):\n",
    "        labels = sequences['labels'].view(-1, 1).to(config.device)\n",
    "        inputs = sequences['concept_ids'].to(config.device), \\\n",
    "            sequences['time_stamps'].to(config.device), \\\n",
    "            sequences['ages'].to(config.device), \\\n",
    "            sequences['visit_orders'].to(config.device), \\\n",
    "            sequences['visit_segments'].to(config.device)\n",
    "\n",
    "        return inputs, labels.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_balanced_accuracy(outputs, labels):\n",
    "        predictions = torch.round(sigmoid(outputs))\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        return balanced_accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters for Bi-LSTM model adn training loop\n",
    "input_size = config.embedding_size          # embedding_dim\n",
    "hidden_size = config.embedding_size // 2    # output hidden size\n",
    "num_layers = 5                              # number of LSTM layers\n",
    "output_size = 1                             # Binary classification, so output size is 1\n",
    "dropout_rate = 0.2                          # Dropout rate for regularization\n",
    "\n",
    "epochs = 6\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_rate).to(config.device)\n",
    "class_weights = torch.tensor([8.0]).to(config.device)  # because ~11% of data is of class 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.7, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_total_loss = 0\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch_no, (sequences, lengths) in tqdm(enumerate(train_loader), file=sys.stdout,\n",
    "                                               total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}',\n",
    "                                               unit=' batch'):\n",
    "        inputs, labels = model.get_inputs_labels(sequences)\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "        # tqdm.write(f'Batch Loss: {loss.item():.4f}', file=sys.stdout, end='\\r')\n",
    "        # print(f'\\nBatch Loss: {loss.item():.4f}', end='\\r')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_no, (sequences, lengths) in tqdm(enumerate(train_loader), file=sys.stdout,\n",
    "                                                   total=len(train_loader),\n",
    "                                                   desc=f'Train Evaluation {epoch + 1}/{epochs}',\n",
    "                                                   unit=' batch'):\n",
    "            inputs, labels = model.get_inputs_labels(sequences)\n",
    "            outputs = model(inputs, lengths)\n",
    "            train_accuracy += model.get_balanced_accuracy(outputs, labels)\n",
    "\n",
    "\n",
    "        for batch_no, (sequences, lengths) in tqdm(enumerate(test_loader), file=sys.stdout,\n",
    "                                                   total=len(test_loader), desc=f'Test Evaluation {epoch + 1}/{epochs}',\n",
    "                                                   unit=' batch'):\n",
    "            inputs, labels = model.get_inputs_labels(sequences)\n",
    "            outputs = model(inputs, lengths)\n",
    "            test_accuracy += model.get_balanced_accuracy(outputs, labels)\n",
    "\n",
    "    print(\n",
    "        f'\\nEpoch {epoch + 1}/{epochs}  |  '\n",
    "        f'Average Train Loss: {train_total_loss / len(train_loader):.5f}  |  '\n",
    "        f'Train Accuracy: {train_accuracy / len(train_loader):.5f}  |  '\n",
    "        f'Test Accuracy: {test_accuracy / len(test_loader):.5f}\\n\\n')\n",
    "    scheduler.step()\n",
    "\n",
    "# torch.save(model, 'LSTM_V2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('/fs01/home/afallah/odyssey/slurm/LSTM_V4_Weighted1.pt').state_dict()\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_rate).to(config.device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_pred = np.array([]); y_train_labels = np.array([])\n",
    "y_test_pred = np.array([]); y_test_labels = np.array([])\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_no, (sequences, lengths) in tqdm(enumerate(train_loader), file=sys.stdout,\n",
    "                                               total=len(train_loader),\n",
    "                                               desc=f'Train Evaluation',\n",
    "                                               unit=' batch'):\n",
    "        inputs, labels = model.get_inputs_labels(sequences)\n",
    "        outputs = model(inputs, lengths)\n",
    "\n",
    "        predictions = torch.round(sigmoid(outputs))\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        y_train_pred = np.append(y_train_pred, predictions)\n",
    "        y_train_labels = np.append(y_train_labels, labels)\n",
    "\n",
    "\n",
    "    for batch_no, (sequences, lengths) in tqdm(enumerate(test_loader), file=sys.stdout,\n",
    "                                               total=len(test_loader), desc=f'Test Evaluation',\n",
    "                                               unit=' batch'):\n",
    "        inputs, labels = model.get_inputs_labels(sequences)\n",
    "        outputs = model(inputs, lengths)\n",
    "\n",
    "        predictions = torch.round(sigmoid(outputs))\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        y_test_pred = np.append(y_test_pred, predictions)\n",
    "        y_test_labels = np.append(y_test_labels, labels)\n",
    "\n",
    "\n",
    "all_data_pred = np.append(y_train_pred, y_test_pred)\n",
    "all_data_labels = np.append(y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ASSESS MODEL PERFORMANCE ###\n",
    "\n",
    "# Balanced Accuracy\n",
    "y_train_accuracy = balanced_accuracy_score(y_train_labels, y_train_pred)\n",
    "y_test_accuracy = balanced_accuracy_score(y_test_labels, y_test_pred)\n",
    "all_data_accuracy = balanced_accuracy_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# F1 Score\n",
    "y_train_f1 = f1_score(y_train_labels, y_train_pred)\n",
    "y_test_f1 = f1_score(y_test_labels, y_test_pred)\n",
    "all_data_f1 = f1_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Precision\n",
    "y_train_precision = precision_score(y_train_labels, y_train_pred)\n",
    "y_test_precision = precision_score(y_test_labels, y_test_pred)\n",
    "all_data_precision = precision_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Recall\n",
    "y_train_recall = recall_score(y_train_labels, y_train_pred)\n",
    "y_test_recall = recall_score(y_test_labels, y_test_pred)\n",
    "all_data_recall = recall_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# AUROC\n",
    "y_train_auroc = roc_auc_score(y_train_labels, y_train_pred)\n",
    "y_test_auroc = roc_auc_score(y_test_labels, y_test_pred)\n",
    "all_data_auroc = roc_auc_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# AUC-PR (Area Under the Precision-Recall Curve)\n",
    "y_train_p, y_train_r, _ = precision_recall_curve(y_train_labels, y_train_pred)\n",
    "y_test_p, y_test_r, _ = precision_recall_curve(y_test_labels, y_test_pred)\n",
    "all_data_p, all_data_r, _ = precision_recall_curve(all_data_labels, all_data_pred)\n",
    "\n",
    "y_train_auc_pr = auc(y_train_r, y_train_p)\n",
    "y_test_auc_pr = auc(y_test_r, y_test_p)\n",
    "all_data_auc_pr = auc(all_data_r, all_data_p)\n",
    "\n",
    "# Average Precision Score (APS)\n",
    "y_train_aps = average_precision_score(y_train_labels, y_train_pred)\n",
    "y_test_aps = average_precision_score(y_test_labels, y_test_pred)\n",
    "all_data_aps = average_precision_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Balanced Accuracy\\nTrain: {y_train_accuracy:.5f}  |  Test: {y_test_accuracy:.5f}  |  All Data: {all_data_accuracy:.5f}\\n\")\n",
    "print(f\"F1 Score\\nTrain: {y_train_f1:.5f}  |  Test: {y_test_f1:.5f}  |  All Data: {all_data_f1:.5f}\\n\")\n",
    "print(f\"Precision\\nTrain: {y_train_precision:.5f}  |  Test: {y_test_precision:.5f}  |  All Data: {all_data_precision:.5f}\\n\")\n",
    "print(f\"Recall\\nTrain: {y_train_recall:.5f}  |  Test: {y_test_recall:.5f}  |  All Data: {all_data_recall:.5f}\\n\")\n",
    "print(f\"AUROC\\nTrain: {y_train_auroc:.5f}  |  Test: {y_test_auroc:.5f}  |  All Data: {all_data_auroc:.5f}\\n\")\n",
    "print(f\"AUC-PR\\nTrain: {y_train_auc_pr:.5f}  |  Test: {y_test_auc_pr:.5f}  |  All Data: {all_data_auc_pr:.5f}\\n\")\n",
    "print(f\"Average Precision Score\\nTrain: {y_train_aps:.5f}  |  Test: {y_test_aps:.5f}  |  All Data: {all_data_aps:.5f}\\n\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train_labels, y_train_pred)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_labels, y_test_pred)\n",
    "fpr_all_data, tpr_all_data, _ = roc_curve(all_data_labels, all_data_pred)\n",
    "\n",
    "# Plot Information\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_train, tpr_train, label=f'Train AUROC={y_train_auroc:.2f}')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test AUROC={y_test_auroc:.2f}')\n",
    "plt.plot(fpr_all_data, tpr_all_data, label=f'All Data AUROC={all_data_auroc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Evaluate 0.82 model, V4_Weighted_1 -> Still imbalanced\n",
    "2. Train + Evaluate high lr model, V4_Weighted\n",
    "3. Train + Evaluate medium lr model with changes, V4_Weighted_2\n",
    "4. Train + Evaluate low lr model with changes, V4_Weighted_3\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
