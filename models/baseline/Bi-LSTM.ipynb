{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:49:56.508433100Z",
     "start_time": "2024-02-09T04:49:56.494432800Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: Bi-LSTM.ipynb\n",
    "Code to train and evaluate a bi-directional LSTM model on MIMIC-IV FHIR dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def Project():\n",
    "    \"\"\"\n",
    "    __Objectives__\n",
    "    0. Import data and tokenizer\n",
    "    1. Train the tokenizer on all sequences of the dataset\n",
    "    2. Tokenize different sequences and join them together\n",
    "    3. Prepare actual labels for one, six, twelve month death after discharge\n",
    "    4. Define the model architecture for bidrectional LSTM\n",
    "    5. Train Bi-LSTM model and evaluate on test dataset\n",
    "    6. Compare performance across new tasks to XGBoost\n",
    "    >>> All objectives successful\n",
    "    \"\"\"\n",
    "    return ProjectObjectives.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:09.043998Z",
     "start_time": "2024-02-09T04:49:56.503433500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 23:50:03.831723: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 23:50:04.033881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 23:50:04.033946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 23:50:04.046401: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 23:50:04.096988: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 23:50:04.098640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 23:50:06.342446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from torch import nn, optim\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.big_bird_cehr.data import FinetuneDataset\n",
    "from models.big_bird_cehr.embeddings import Embeddings\n",
    "from models.big_bird_cehr.tokenizer import ConceptTokenizer\n",
    "\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/512/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/pretrain.parquet\"\n",
    "FINE_TUNE_PATH = f\"{DATA_ROOT}/fine_tune.parquet\"\n",
    "TEST_DATA_PATH = f\"{DATA_ROOT}/fine_test.parquet\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:09.149160700Z",
     "start_time": "2024-02-09T04:50:09.043998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# save parameters and configurations\n",
    "class config:\n",
    "    \"\"\"A simple class to store all configurations\"\"\"\n",
    "\n",
    "    seed = 23\n",
    "    data_dir = DATA_ROOT\n",
    "    test_size = 0.2\n",
    "    batch_size = 64\n",
    "    num_workers = 3\n",
    "    vocab_size = None\n",
    "    embedding_size = 768\n",
    "    time_embeddings_size = 32\n",
    "    type_vocab_size = 8\n",
    "    max_len = 512\n",
    "    padding_idx = None\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Seed all parts of the training process\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(config.seed)\n",
    "print(f\"Cuda: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.198821600Z",
     "start_time": "2024-02-09T04:50:09.150156900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_512</th>\n",
       "      <th>type_tokens_512</th>\n",
       "      <th>age_tokens_512</th>\n",
       "      <th>time_tokens_512</th>\n",
       "      <th>visit_tokens_512</th>\n",
       "      <th>position_tokens_512</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f8f3289a-057f-5fcc-a714-5f6109ca16c4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[CLS], [VS], 8938, [VE], [PAD], [PAD], [PAD],...</td>\n",
       "      <td>[1, 2, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 18, 18, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 8262, 8262, 8262, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 2, 2, 2, 513, 513, 513, 513, 513, 513, 513...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9b62c9f4-3fdc-5020-82b5-ae5b8292445a</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>[[CLS], [VS], 7569, 66689036430, 00904224461, ...</td>\n",
       "      <td>[1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 4, ...</td>\n",
       "      <td>[0, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28...</td>\n",
       "      <td>[0, 5963, 5963, 5963, 5963, 5963, 5963, 5963, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ca522eb-dd89-5f79-8155-9599ea46b0b2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>244.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>51</td>\n",
       "      <td>54</td>\n",
       "      <td>[[CLS], [VS], 00904629261, 00904642281, 009046...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86...</td>\n",
       "      <td>[0, 8016, 8016, 8016, 8016, 8016, 8016, 8016, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02adf8a6-8bc0-55d3-81ae-4d8582094896</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>640</td>\n",
       "      <td>664</td>\n",
       "      <td>[[CLS], [VS], 52007_3, 51476_2, 50861_1, 50862...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73...</td>\n",
       "      <td>[0, 8426, 8426, 8426, 8426, 8426, 8426, 8426, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>744fe3c4-9b03-55ae-ac9f-6bc4e967cde7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>[[CLS], [VS], 7813, 7813, 7902, 7902, 9604, 00...</td>\n",
       "      <td>[1, 2, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...</td>\n",
       "      <td>[0, 7582, 7582, 7582, 7582, 7582, 7582, 7582, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59898</th>\n",
       "      <td>3dafdc7c-c80b-56f0-832a-3ab7bf5667cc</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1956</td>\n",
       "      <td>1992</td>\n",
       "      <td>[[CLS], [VS], 51379_3, 51376_1, 51375_3, 51516...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81...</td>\n",
       "      <td>[0, 9072, 9072, 9072, 9072, 9072, 9072, 9072, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164563</th>\n",
       "      <td>17cd52f2-9a32-5b2d-aec8-bbaefce55e7b</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1877</td>\n",
       "      <td>1886</td>\n",
       "      <td>[[CLS], [VS], 50863_1, 50868_3, 50878_4, 50882...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58...</td>\n",
       "      <td>[0, 6777, 6777, 6777, 6777, 6777, 6777, 6777, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>052ca40b-d12e-5390-a9f1-70a6edbfa162</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1879</td>\n",
       "      <td>1882</td>\n",
       "      <td>[[CLS], [VS], 50902_0, 50912_4, 50931_4, 50960...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89...</td>\n",
       "      <td>[0, 6083, 6083, 6083, 6083, 6083, 6083, 6083, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28930</th>\n",
       "      <td>472730fd-189f-524c-a649-1f9d184e80c7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1090</td>\n",
       "      <td>1105</td>\n",
       "      <td>[[CLS], [VS], 51250_4, 51248_2, 51244_2, 51222...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81...</td>\n",
       "      <td>[0, 6604, 6604, 6604, 6604, 6604, 6604, 6604, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64515</th>\n",
       "      <td>8b689ccf-1b9e-578c-be35-c4c9887d886c</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>[[CLS], [VS], 9623, 58914050156, 00338011704, ...</td>\n",
       "      <td>[1, 2, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41...</td>\n",
       "      <td>[0, 8593, 8593, 8593, 8593, 8593, 8593, 8593, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170593 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  patient_id  num_visits  deceased  \\\n",
       "index                                                                \n",
       "0       f8f3289a-057f-5fcc-a714-5f6109ca16c4           2         0   \n",
       "1       9b62c9f4-3fdc-5020-82b5-ae5b8292445a           4         0   \n",
       "2       2ca522eb-dd89-5f79-8155-9599ea46b0b2           2         1   \n",
       "4       02adf8a6-8bc0-55d3-81ae-4d8582094896           9         1   \n",
       "5       744fe3c4-9b03-55ae-ac9f-6bc4e967cde7           3         0   \n",
       "...                                      ...         ...       ...   \n",
       "59898   3dafdc7c-c80b-56f0-832a-3ab7bf5667cc          13         1   \n",
       "164563  17cd52f2-9a32-5b2d-aec8-bbaefce55e7b           4         1   \n",
       "22291   052ca40b-d12e-5390-a9f1-70a6edbfa162           2         1   \n",
       "28930   472730fd-189f-524c-a649-1f9d184e80c7           6         1   \n",
       "64515   8b689ccf-1b9e-578c-be35-c4c9887d886c           3         0   \n",
       "\n",
       "        death_after_start  death_after_end  length  token_length  \\\n",
       "index                                                              \n",
       "0                     NaN              NaN       1             4   \n",
       "1                     NaN              NaN      43            52   \n",
       "2                   244.0            242.0      51            54   \n",
       "4                    20.0             11.0     640           664   \n",
       "5                     NaN              NaN      80            86   \n",
       "...                   ...              ...     ...           ...   \n",
       "59898                11.0             10.0    1956          1992   \n",
       "164563               16.0              5.0    1877          1886   \n",
       "22291                63.0             24.0    1879          1882   \n",
       "28930                 2.0              0.0    1090          1105   \n",
       "64515                 NaN              NaN      53            59   \n",
       "\n",
       "                                         event_tokens_512  \\\n",
       "index                                                       \n",
       "0       [[CLS], [VS], 8938, [VE], [PAD], [PAD], [PAD],...   \n",
       "1       [[CLS], [VS], 7569, 66689036430, 00904224461, ...   \n",
       "2       [[CLS], [VS], 00904629261, 00904642281, 009046...   \n",
       "4       [[CLS], [VS], 52007_3, 51476_2, 50861_1, 50862...   \n",
       "5       [[CLS], [VS], 7813, 7813, 7902, 7902, 9604, 00...   \n",
       "...                                                   ...   \n",
       "59898   [[CLS], [VS], 51379_3, 51376_1, 51375_3, 51516...   \n",
       "164563  [[CLS], [VS], 50863_1, 50868_3, 50878_4, 50882...   \n",
       "22291   [[CLS], [VS], 50902_0, 50912_4, 50931_4, 50960...   \n",
       "28930   [[CLS], [VS], 51250_4, 51248_2, 51244_2, 51222...   \n",
       "64515   [[CLS], [VS], 9623, 58914050156, 00338011704, ...   \n",
       "\n",
       "                                          type_tokens_512  \\\n",
       "index                                                       \n",
       "0       [1, 2, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 4, ...   \n",
       "2       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4       [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "5       [1, 2, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "...                                                   ...   \n",
       "59898   [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "164563  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "22291   [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "28930   [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "64515   [1, 2, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "\n",
       "                                           age_tokens_512  \\\n",
       "index                                                       \n",
       "0       [0, 18, 18, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [0, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28...   \n",
       "2       [0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86...   \n",
       "4       [0, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73...   \n",
       "5       [0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...   \n",
       "...                                                   ...   \n",
       "59898   [0, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81...   \n",
       "164563  [0, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58...   \n",
       "22291   [0, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89...   \n",
       "28930   [0, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81...   \n",
       "64515   [0, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41...   \n",
       "\n",
       "                                          time_tokens_512  \\\n",
       "index                                                       \n",
       "0       [0, 8262, 8262, 8262, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [0, 5963, 5963, 5963, 5963, 5963, 5963, 5963, ...   \n",
       "2       [0, 8016, 8016, 8016, 8016, 8016, 8016, 8016, ...   \n",
       "4       [0, 8426, 8426, 8426, 8426, 8426, 8426, 8426, ...   \n",
       "5       [0, 7582, 7582, 7582, 7582, 7582, 7582, 7582, ...   \n",
       "...                                                   ...   \n",
       "59898   [0, 9072, 9072, 9072, 9072, 9072, 9072, 9072, ...   \n",
       "164563  [0, 6777, 6777, 6777, 6777, 6777, 6777, 6777, ...   \n",
       "22291   [0, 6083, 6083, 6083, 6083, 6083, 6083, 6083, ...   \n",
       "28930   [0, 6604, 6604, 6604, 6604, 6604, 6604, 6604, ...   \n",
       "64515   [0, 8593, 8593, 8593, 8593, 8593, 8593, 8593, ...   \n",
       "\n",
       "                                         visit_tokens_512  \\\n",
       "index                                                       \n",
       "0       [0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, ...   \n",
       "2       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "5       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                   ...   \n",
       "59898   [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "164563  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "22291   [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "28930   [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "64515   [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                      position_tokens_512  label  \n",
       "index                                                             \n",
       "0       [0, 2, 2, 2, 513, 513, 513, 513, 513, 513, 513...      0  \n",
       "1       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "2       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "4       [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...      1  \n",
       "5       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "...                                                   ...    ...  \n",
       "59898   [1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...      1  \n",
       "164563  [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...      1  \n",
       "22291   [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...      1  \n",
       "28930   [1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...      1  \n",
       "64515   [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "\n",
       "[170593 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "pretrain_data = pd.read_parquet(DATA_PATH)\n",
    "pretrain_data = pretrain_data[pretrain_data[\"event_tokens_512\"].notnull()]\n",
    "\n",
    "finetune_data = pd.read_parquet(FINE_TUNE_PATH)\n",
    "finetune_data = finetune_data[finetune_data[\"event_tokens_512\"].notnull()]\n",
    "\n",
    "test_data = pd.read_parquet(TEST_DATA_PATH)\n",
    "test_data = test_data[test_data[\"event_tokens_512\"].notnull()]\n",
    "test_length = len(test_data)\n",
    "\n",
    "train_data = pd.concat((pretrain_data, finetune_data))\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop_duplicates(subset=\"index\", keep=\"first\", inplace=True).set_index(\"index\")\n",
    "\n",
    "del pretrain_data, finetune_data\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.209725Z",
     "start_time": "2024-02-09T04:50:22.200726900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define custom labels if dataset does not already have labels\n",
    "# data[\"label\"] = (\n",
    "#     (data[\"death_after_end\"] >= 0) & (data[\"death_after_end\"] < 365)\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.369872300Z",
     "start_time": "2024-02-09T04:50:22.212728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.big_bird_cehr.tokenizer.ConceptTokenizer at 0x7f756f4433a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit tokenizer on .json vocab files\n",
    "tokenizer = ConceptTokenizer(data_dir=config.data_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "config.vocab_size = tokenizer.get_vocab_size()\n",
    "config.padding_idx = tokenizer.get_pad_token_id()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.494688Z",
     "start_time": "2024-02-09T04:50:22.424633300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready to go!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define dataset with token lengths\n",
    "class DatasetWithTokenLength(Dataset):\n",
    "    def __init__(self, tokenized_data: FinetuneDataset, length_data: np.ndarray):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.length_data = length_data\n",
    "\n",
    "        assert len(tokenized_data) == len(length_data), \"Datasets have different lengths\"\n",
    "\n",
    "        self.sorted_indices = sorted(\n",
    "            range(len(length_data)), key=lambda x: length_data[x], reverse=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, int]:\n",
    "        index = self.sorted_indices[index]\n",
    "        return self.tokenized_data[index], min(config.max_len, self.length_data[index])\n",
    "\n",
    "\n",
    "# Get training and test datasets\n",
    "\n",
    "# Split data if needed\n",
    "# train_data, test_data = train_test_split(\n",
    "#     data, test_size=config.test_size, random_state=config.seed, stratify=data[\"label\"]\n",
    "# )\n",
    "\n",
    "# Get train and test datasets and dataloaders\n",
    "train_dataset = FinetuneDataset(\n",
    "    data=train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len,\n",
    ")\n",
    "\n",
    "test_dataset = FinetuneDataset(\n",
    "    data=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config.max_len,\n",
    ")\n",
    "\n",
    "train_dataset_with_lengths = DatasetWithTokenLength(\n",
    "    train_dataset, train_data[\"token_length\"].values,\n",
    ")\n",
    "test_dataset_with_lengths = DatasetWithTokenLength(\n",
    "    test_dataset, test_data[\"token_length\"].values,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_with_lengths,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_with_lengths,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Data is ready to go!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.503675700Z",
     "start_time": "2024-02-09T04:50:22.494688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            hidden_size: int,\n",
    "            num_layers: int,\n",
    "            output_size: int,\n",
    "            dropout_rate: float,\n",
    "    ):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.embedding_size,\n",
    "            time_embedding_size=config.time_embeddings_size,\n",
    "            type_vocab_size=config.type_vocab_size,\n",
    "            max_len=config.max_len,\n",
    "            padding_idx=config.padding_idx,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, inputs: Tuple[Any], lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of Bi-LSTM model\"\"\"\n",
    "        embed = self.embeddings(*inputs)\n",
    "        packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(packed_embed)\n",
    "        output = torch.cat((hidden_state[-2, :, :], hidden_state[-1, :, :]), dim=1)\n",
    "\n",
    "        output = self.dropout(self.batch_norm(output))\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_inputs_labels(sequences: Dict[str, torch.Tensor]) -> Tuple[Any, torch.Tensor]:\n",
    "        \"\"\"Create inputs tuples from a dictionary of sequences\"\"\"\n",
    "        labels = sequences[\"labels\"].view(-1, 1).to(config.device)\n",
    "        inputs = (\n",
    "            sequences[\"concept_ids\"].to(config.device),\n",
    "            sequences[\"type_ids\"].to(config.device),\n",
    "            sequences[\"time_stamps\"].to(config.device),\n",
    "            sequences[\"ages\"].to(config.device),\n",
    "            sequences[\"visit_orders\"].to(config.device),\n",
    "            sequences[\"visit_segments\"].to(config.device),\n",
    "        )\n",
    "\n",
    "        return inputs, labels.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_balanced_accuracy(outputs: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        \"\"\"Return the balanced accuracy metric by comparing outputs to labels\"\"\"\n",
    "        predictions = torch.round(sigmoid(outputs))\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        return balanced_accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T04:50:22.515675900Z",
     "start_time": "2024-02-09T04:50:22.514675600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters for Bi-LSTM model adn training loop\n",
    "input_size = config.embedding_size  # embedding_dim\n",
    "hidden_size = config.embedding_size // 2  # output hidden size\n",
    "num_layers = 5  # number of LSTM layers\n",
    "output_size = 1  # Binary classification, so output size is 1\n",
    "dropout_rate = 0.2  # Dropout rate for regularization\n",
    "\n",
    "# Set training hyperparameters\n",
    "epochs = 6\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T05:58:12.850427800Z",
     "start_time": "2024-02-09T04:50:22.518700100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 1/6: 100%|██████████| 2666/2666 [11:34<00:00,  3.84 batch/s]\n",
      "\n",
      "Epoch 1/6  |  Average Train Loss: 1.04758  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 7.5000e-04.\n",
      "Epoch 2/6: 100%|██████████| 2666/2666 [11:32<00:00,  3.85 batch/s]\n",
      "\n",
      "Epoch 2/6  |  Average Train Loss: 0.79291  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 5.6250e-04.\n",
      "Epoch 3/6: 100%|██████████| 2666/2666 [11:21<00:00,  3.91 batch/s]\n",
      "\n",
      "Epoch 3/6  |  Average Train Loss: 0.72944  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 4.2188e-04.\n",
      "Epoch 4/6: 100%|██████████| 2666/2666 [11:10<00:00,  3.98 batch/s]\n",
      "\n",
      "Epoch 4/6  |  Average Train Loss: 0.70437  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 3.1641e-04.\n",
      "Epoch 5/6: 100%|██████████| 2666/2666 [11:06<00:00,  4.00 batch/s]\n",
      "\n",
      "Epoch 5/6  |  Average Train Loss: 0.68042  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 2.3730e-04.\n",
      "Epoch 6/6: 100%|██████████| 2666/2666 [10:55<00:00,  4.07 batch/s]\n",
      "\n",
      "Epoch 6/6  |  Average Train Loss: 0.66466  |  Train Accuracy: 0.00000  |  \n",
      "Adjusting learning rate of group 0 to 1.7798e-04.\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_rate).to(\n",
    "    config.device,\n",
    ")\n",
    "class_weights = torch.tensor([6]).to(config.device) # Determined with experiment\n",
    "loss_fcn = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.75, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_total_loss = 0\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch_no, (sequences, lengths) in tqdm(\n",
    "        enumerate(train_loader),\n",
    "        file=sys.stdout,\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "        unit=\" batch\",\n",
    "    ):\n",
    "        inputs, labels = model.get_inputs_labels(sequences)\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = loss_fcn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "\n",
    "    # Run an evaluation loop if needed\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_no, (sequences, lengths) in tqdm(\n",
    "    #         enumerate(train_loader),\n",
    "    #         file=sys.stdout,\n",
    "    #         total=len(train_loader),\n",
    "    #         desc=f\"Train Evaluation {epoch + 1}/{epochs}\",\n",
    "    #         unit=\" batch\",\n",
    "    #     ):\n",
    "    #         inputs, labels = model.get_inputs_labels(sequences)\n",
    "    #         outputs = model(inputs, lengths)\n",
    "    #         train_accuracy += model.get_balanced_accuracy(outputs, labels)\n",
    "    #\n",
    "    #     for batch_no, (sequences, lengths) in tqdm(\n",
    "    #         enumerate(test_loader),\n",
    "    #         file=sys.stdout,\n",
    "    #         total=len(test_loader),\n",
    "    #         desc=f\"Test Evaluation {epoch + 1}/{epochs}\",\n",
    "    #         unit=\" batch\",\n",
    "    #     ):\n",
    "    #         inputs, labels = model.get_inputs_labels(sequences)\n",
    "    #         outputs = model(inputs, lengths)\n",
    "    #         test_accuracy += model.get_balanced_accuracy(outputs, labels)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1}/{epochs}\"\n",
    "        f\"  |  Average Train Loss: {train_total_loss / len(train_loader):.5f}\"\n",
    "        f\"  |  Train Accuracy: {train_accuracy / len(train_loader):.5f}\"\n",
    "        # f\"  |  Test Accuracy: {test_accuracy / len(test_loader):.5f}\\n\\n\"\n",
    "        , \"\\n\\n\",\n",
    "    )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the model if needed\n",
    "# torch.save(model, 'LSTM_V2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model from disk\n",
    "state_dict = torch.load(\n",
    "    \"/fs01/home/afallah/odyssey/slurm/LSTM_V4_Weighted1.pt\",\n",
    ").state_dict()\n",
    "\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, output_size, dropout_rate).to(\n",
    "    config.device,\n",
    ")\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T05:58:20.707426Z",
     "start_time": "2024-02-09T05:58:12.855429600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation: 100%|██████████| 47/47 [00:07<00:00,  6.45 batch/s]\n"
     ]
    }
   ],
   "source": [
    "# Assess model performance and get predictions for train and test set\n",
    "y_train_pred = np.array([])\n",
    "y_train_labels = np.array([])\n",
    "y_test_pred = np.array([])\n",
    "y_test_labels = np.array([])\n",
    "y_test_prob = np.array([])\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Get predictions on training data\n",
    "    # for batch_no, (sequences, lengths) in tqdm(\n",
    "    #     enumerate(train_loader),\n",
    "    #     file=sys.stdout,\n",
    "    #     total=len(train_loader),\n",
    "    #     desc=\"Train Evaluation\",\n",
    "    #     unit=\" batch\",\n",
    "    # ):\n",
    "    #     inputs, labels = model.get_inputs_labels(sequences)\n",
    "    #     outputs = model(inputs, lengths)\n",
    "    #\n",
    "    #     predictions = torch.round(sigmoid(outputs))\n",
    "    #     predictions = predictions.detach().cpu().numpy()\n",
    "    #     labels = labels.detach().cpu().numpy()\n",
    "    #\n",
    "    #     y_train_pred = np.append(y_train_pred, predictions)\n",
    "    #     y_train_labels = np.append(y_train_labels, labels)\n",
    "\n",
    "    # Get predictions on test data\n",
    "    for batch_no, (sequences, lengths) in tqdm(\n",
    "        enumerate(test_loader),\n",
    "        file=sys.stdout,\n",
    "        total=len(test_loader),\n",
    "        desc=\"Test Evaluation\",\n",
    "        unit=\" batch\",\n",
    "    ):\n",
    "        inputs, labels = model.get_inputs_labels(sequences)\n",
    "        outputs = model(inputs, lengths).detach().cpu()\n",
    "\n",
    "        predictions = torch.round(sigmoid(outputs))\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        y_test_pred = np.append(y_test_pred, predictions)\n",
    "        y_test_labels = np.append(y_test_labels, labels)\n",
    "        y_test_prob = np.append(y_test_prob, sigmoid(outputs))\n",
    "\n",
    "# Save test and training data prediction together\n",
    "# all_data_pred = np.append(y_train_pred, y_test_pred)\n",
    "# all_data_labels = np.append(y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T05:58:20.715428Z",
     "start_time": "2024-02-09T05:58:20.707426Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model predictions, labels, and probabilities to disk\n",
    "np.save(f\"{ROOT}/lstm_y_test_pred_one_month.npy\", y_test_pred)\n",
    "np.save(f\"{ROOT}/lstm_y_test_pred_one_month_labels.npy\", y_test_labels)\n",
    "np.save(f\"{ROOT}/lstm_y_test_pred_one_month_prob.npy\", y_test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSESS MODEL PERFORMANCE ###\n",
    "\n",
    "# Balanced Accuracy\n",
    "y_train_accuracy = balanced_accuracy_score(y_train_labels, y_train_pred)\n",
    "y_test_accuracy = balanced_accuracy_score(y_test_labels, y_test_pred)\n",
    "all_data_accuracy = balanced_accuracy_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# F1 Score\n",
    "y_train_f1 = f1_score(y_train_labels, y_train_pred)\n",
    "y_test_f1 = f1_score(y_test_labels, y_test_pred)\n",
    "all_data_f1 = f1_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Precision\n",
    "y_train_precision = precision_score(y_train_labels, y_train_pred)\n",
    "y_test_precision = precision_score(y_test_labels, y_test_pred)\n",
    "all_data_precision = precision_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Recall\n",
    "y_train_recall = recall_score(y_train_labels, y_train_pred)\n",
    "y_test_recall = recall_score(y_test_labels, y_test_pred)\n",
    "all_data_recall = recall_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# AUROC\n",
    "y_train_auroc = roc_auc_score(y_train_labels, y_train_pred)\n",
    "y_test_auroc = roc_auc_score(y_test_labels, y_test_pred)\n",
    "all_data_auroc = roc_auc_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# AUC-PR (Area Under the Precision-Recall Curve)\n",
    "y_train_p, y_train_r, _ = precision_recall_curve(y_train_labels, y_train_pred)\n",
    "y_test_p, y_test_r, _ = precision_recall_curve(y_test_labels, y_test_pred)\n",
    "all_data_p, all_data_r, _ = precision_recall_curve(all_data_labels, all_data_pred)\n",
    "\n",
    "y_train_auc_pr = auc(y_train_r, y_train_p)\n",
    "y_test_auc_pr = auc(y_test_r, y_test_p)\n",
    "all_data_auc_pr = auc(all_data_r, all_data_p)\n",
    "\n",
    "# Average Precision Score (APS)\n",
    "y_train_aps = average_precision_score(y_train_labels, y_train_pred)\n",
    "y_test_aps = average_precision_score(y_test_labels, y_test_pred)\n",
    "all_data_aps = average_precision_score(all_data_labels, all_data_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(\n",
    "    f\"Balanced Accuracy\\nTrain: {y_train_accuracy:.5f}  |  Test: {y_test_accuracy:.5f}  |  All Data: {all_data_accuracy:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"F1 Score\\nTrain: {y_train_f1:.5f}  |  Test: {y_test_f1:.5f}  |  All Data: {all_data_f1:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Precision\\nTrain: {y_train_precision:.5f}  |  Test: {y_test_precision:.5f}  |  All Data: {all_data_precision:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Recall\\nTrain: {y_train_recall:.5f}  |  Test: {y_test_recall:.5f}  |  All Data: {all_data_recall:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"AUROC\\nTrain: {y_train_auroc:.5f}  |  Test: {y_test_auroc:.5f}  |  All Data: {all_data_auroc:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"AUC-PR\\nTrain: {y_train_auc_pr:.5f}  |  Test: {y_test_auc_pr:.5f}  |  All Data: {all_data_auc_pr:.5f}\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Average Precision Score\\nTrain: {y_train_aps:.5f}  |  Test: {y_test_aps:.5f}  |  All Data: {all_data_aps:.5f}\\n\",\n",
    ")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train_labels, y_train_pred)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_labels, y_test_pred)\n",
    "fpr_all_data, tpr_all_data, _ = roc_curve(all_data_labels, all_data_pred)\n",
    "\n",
    "# Plot Information\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train AUROC={y_train_auroc:.2f}\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test AUROC={y_test_auroc:.2f}\")\n",
    "plt.plot(fpr_all_data, tpr_all_data, label=f\"All Data AUROC={all_data_auroc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
