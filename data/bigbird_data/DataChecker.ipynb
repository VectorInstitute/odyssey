{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:14:45.546088300Z",
     "start_time": "2024-03-13T16:14:43.587090300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from os.path import join\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "DATA_ROOT = '/h/afallah/odyssey/odyssey/data/bigbird_data'\n",
    "DATASET = f'{DATA_ROOT}/patient_sequences/patient_sequences_2048.parquet'\n",
    "MAX_LEN = 2048\n",
    "\n",
    "SEED = 23\n",
    "os.chdir(DATA_ROOT)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:12.321718600Z",
     "start_time": "2024-03-13T16:14:45.553089800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: Index(['patient_id', 'num_visits', 'deceased', 'death_after_start',\n",
      "       'death_after_end', 'length', 'token_length', 'event_tokens_2048',\n",
      "       'type_tokens_2048', 'age_tokens_2048', 'time_tokens_2048',\n",
      "       'visit_tokens_2048', 'position_tokens_2048', 'elapsed_tokens_2048',\n",
      "       'common_conditions', 'rare_conditions'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_2048</th>\n",
       "      <th>type_tokens_2048</th>\n",
       "      <th>age_tokens_2048</th>\n",
       "      <th>time_tokens_2048</th>\n",
       "      <th>visit_tokens_2048</th>\n",
       "      <th>position_tokens_2048</th>\n",
       "      <th>elapsed_tokens_2048</th>\n",
       "      <th>common_conditions</th>\n",
       "      <th>rare_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35581927-9c95-5ae9-af76-7d74870a349c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>[[CLS], [VS], 00006473900, 00904516561, 510790...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...</td>\n",
       "      <td>[0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...</td>\n",
       "      <td>[0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f5bba8dd-25c0-5336-8d3d-37424c185026</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>156</td>\n",
       "      <td>[[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...</td>\n",
       "      <td>[0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4938f91-cadb-5133-8541-a52fb0916cea</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>[[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...</td>\n",
       "      <td>[1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...</td>\n",
       "      <td>[0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe2371b-a6f0-5436-aade-7795005b0c66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>[[CLS], [VS], 63739057310, 49281041688, 005970...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...</td>\n",
       "      <td>[0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6f7590ae-f3b9-50e5-9e41-d4bb1000887a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>[[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...</td>\n",
       "      <td>[0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             patient_id  num_visits  deceased  \\\n",
       "0  35581927-9c95-5ae9-af76-7d74870a349c           1         0   \n",
       "1  f5bba8dd-25c0-5336-8d3d-37424c185026           2         0   \n",
       "2  f4938f91-cadb-5133-8541-a52fb0916cea           2         0   \n",
       "3  6fe2371b-a6f0-5436-aade-7795005b0c66           2         0   \n",
       "4  6f7590ae-f3b9-50e5-9e41-d4bb1000887a           1         0   \n",
       "\n",
       "   death_after_start  death_after_end  length  token_length  \\\n",
       "0                NaN              NaN      50            54   \n",
       "1                NaN              NaN     148           156   \n",
       "2                NaN              NaN      78            86   \n",
       "3                NaN              NaN      86            94   \n",
       "4                NaN              NaN      72            76   \n",
       "\n",
       "                                   event_tokens_2048  \\\n",
       "0  [[CLS], [VS], 00006473900, 00904516561, 510790...   \n",
       "1  [[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...   \n",
       "2  [[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...   \n",
       "3  [[CLS], [VS], 63739057310, 49281041688, 005970...   \n",
       "4  [[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...   \n",
       "\n",
       "                                    type_tokens_2048  \\\n",
       "0  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...   \n",
       "1  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2  [1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "3  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "\n",
       "                                     age_tokens_2048  \\\n",
       "0  [0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...   \n",
       "1  [0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...   \n",
       "2  [0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...   \n",
       "3  [0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...   \n",
       "4  [0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...   \n",
       "\n",
       "                                    time_tokens_2048  \\\n",
       "0  [0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...   \n",
       "1  [0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...   \n",
       "2  [0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...   \n",
       "3  [0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...   \n",
       "4  [0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...   \n",
       "\n",
       "                                   visit_tokens_2048  \\\n",
       "0  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                position_tokens_2048  \\\n",
       "0  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                 elapsed_tokens_2048  \\\n",
       "0  [-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...   \n",
       "1  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  [-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...   \n",
       "3  [-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n",
       "4  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                common_conditions                 rare_conditions  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "# dataset_2048.drop(\n",
    "#     ['event_tokens', 'type_tokens', 'age_tokens', 'time_tokens', 'visit_tokens', 'position_tokens'],\n",
    "#     axis=1,\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "print(f'Current columns: {dataset_2048.columns}')\n",
    "dataset_2048.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ids = pickle.load(open('new_data/patient_id_dict/sample_pretrain_test_patient_ids_with_conditions.pkl', 'rb'))['test']\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(test_ids)]['rare_conditions'].transform(lambda x: x[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_and_pad(row: pd.Series) -> Any:\n",
    "    \"\"\"Return a truncated and padded version of row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        Any: The truncated and padded row.\n",
    "\n",
    "    Note:\n",
    "        This function assumes the presence of the following columns in row:\n",
    "        - 'event_tokens_2048'\n",
    "        - 'type_tokens_2048'\n",
    "        - 'age_tokens_2048'\n",
    "        - 'time_tokens_2048'\n",
    "        - 'visit_tokens_2048'\n",
    "        - 'position_tokens_2048'\n",
    "        - 'elapsed_tokens_2048'\n",
    "    \"\"\"\n",
    "    seq_len = len(row['event_tokens_2048'])\n",
    "    row['type_tokens_2048'] = np.pad(row['type_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['age_tokens_2048'] = np.pad(row['age_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['time_tokens_2048'] = np.pad(row['time_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['visit_tokens_2048'] = np.pad(row['visit_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['position_tokens_2048'] = np.pad(row['position_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['elapsed_tokens_2048'] = np.pad(row['elapsed_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    return row\n",
    "\n",
    "\n",
    "def filter_by_num_visit(dataset: pd.DataFrame, minimum_num_visits: int) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on num_visits threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold num_visits\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['num_visits'] >= minimum_num_visits]\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def filter_by_length_of_stay(dataset: pd.DataFrame, threshold: int = 1) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on length of stay threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold length of stay\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['length_of_stay'] >= threshold]\n",
    "\n",
    "    # Only keep the patients that their first event happens within threshold\n",
    "    filtered_dataset = filtered_dataset[\n",
    "        filtered_dataset.apply(\n",
    "        lambda row: row['elapsed_tokens_2048'][row['last_VS_index'] + 1] < threshold*24,\n",
    "        axis=1)]\n",
    "\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def get_last_index(seq: List[str], target: str) -> int:\n",
    "    \"\"\"Return the index of the last occurrence of target in seq.\n",
    "\n",
    "    Args:\n",
    "        seq (List[str]): The input sequence.\n",
    "        target (str): The target string to find.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the last occurrence of target in seq.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_last_index(['A', 'B', 'A', 'A', 'C', 'D'], 'A')\n",
    "        3\n",
    "    \"\"\"\n",
    "    return len(seq) - (seq[::-1].index(target) + 1)\n",
    "\n",
    "\n",
    "def check_readmission_label(row: pd.Series) -> int:\n",
    "    \"\"\"Check if the label indicates readmission within one month.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if readmission label is present, False otherwise.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return int(row['event_tokens_2048'][last_vs_index - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'))\n",
    "\n",
    "\n",
    "def remove_last_visit(row: pd.Series) -> pd.Series:\n",
    "    \"\"\" Remove the event tokens of last visit in the row\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return row['event_tokens_2048'][:last_vs_index - 1]\n",
    "\n",
    "\n",
    "def get_length_of_stay(row: pd.Series) -> pd.Series:\n",
    "    \"\"\" Determine the length of a given visit. \n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.    \n",
    "    \"\"\"\n",
    "    admission_time = row['last_VS_index'] + 1\n",
    "    discharge_time = row['last_VE_index'] - 1\n",
    "    return (discharge_time - admission_time) / 24\n",
    "\n",
    "\n",
    "def truncate_visit_after_threshold(row: pd.Series, threshold: int = 24) -> pd.Series:\n",
    "    \"\"\" Remove the event tokens of last visit that occur after threshold hours.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "        threshold (int): The cut of threshold.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    last_ve_index = row['last_VE_index']\n",
    "\n",
    "    for i in range(last_vs_index+1, last_ve_index):\n",
    "        if row['elapsed_tokens_2048'][i] > threshold:\n",
    "            return row['event_tokens_2048'][:i]\n",
    "    \n",
    "    return row['event_tokens_2048']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_2048</th>\n",
       "      <th>type_tokens_2048</th>\n",
       "      <th>age_tokens_2048</th>\n",
       "      <th>time_tokens_2048</th>\n",
       "      <th>visit_tokens_2048</th>\n",
       "      <th>position_tokens_2048</th>\n",
       "      <th>elapsed_tokens_2048</th>\n",
       "      <th>common_conditions</th>\n",
       "      <th>rare_conditions</th>\n",
       "      <th>last_VS_index</th>\n",
       "      <th>last_VE_index</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>label_length_of_stay_1week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35581927-9c95-5ae9-af76-7d74870a349c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>[CLS] [VS] 00006473900 00904516561 51079000220...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...</td>\n",
       "      <td>[0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...</td>\n",
       "      <td>[0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f5bba8dd-25c0-5336-8d3d-37424c185026</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>81</td>\n",
       "      <td>[CLS] [VS] 52135_2 52075_2 52074_2 52073_3 520...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...</td>\n",
       "      <td>[0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>68</td>\n",
       "      <td>154</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4938f91-cadb-5133-8541-a52fb0916cea</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>[CLS] [VS] 0RB30ZZ 0RG10A0 00071101441 0090419...</td>\n",
       "      <td>[1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...</td>\n",
       "      <td>[0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>49</td>\n",
       "      <td>84</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe2371b-a6f0-5436-aade-7795005b0c66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>[CLS] [VS] 63739057310 49281041688 00597026010...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...</td>\n",
       "      <td>[0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>55</td>\n",
       "      <td>92</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6f7590ae-f3b9-50e5-9e41-d4bb1000887a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>56</td>\n",
       "      <td>[CLS] [VS] 50813_0 52135_0 52075_3 52074_3 520...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...</td>\n",
       "      <td>[0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>2.958333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143474</th>\n",
       "      <td>3f300d4e-4554-5f1f-9dff-f209a4916cbc</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>536</td>\n",
       "      <td>564</td>\n",
       "      <td>[CLS] [VS] 51484_0 51146_3 51200_1 51221_4 512...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64...</td>\n",
       "      <td>[0, 6921, 6921, 6921, 6921, 6921, 6921, 6921, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>520</td>\n",
       "      <td>562</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143475</th>\n",
       "      <td>cf2115d7-937e-511d-b159-dd7eb3d5d420</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>142</td>\n",
       "      <td>[CLS] [VS] 33332001001 00781305714 10019017644...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 5, ...</td>\n",
       "      <td>[0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32...</td>\n",
       "      <td>[0, 5481, 5481, 5481, 5481, 5481, 5481, 5481, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.16, 1.25, 1.3, 1.3, 1.31, 1.53,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>109</td>\n",
       "      <td>172</td>\n",
       "      <td>2.541667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143476</th>\n",
       "      <td>31338a39-28f9-54a5-a810-2d05fbaa5166</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283</td>\n",
       "      <td>221</td>\n",
       "      <td>[CLS] [VS] 00338011704 00409128331 63323026201...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50...</td>\n",
       "      <td>[0, 4997, 4997, 4997, 4997, 4997, 4997, 4997, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.48, 1.49, 1.52, 1.52, 1.6, 1.6,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>192</td>\n",
       "      <td>293</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143477</th>\n",
       "      <td>0989415d-394c-5f42-8dac-75dc7306a23c</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470</td>\n",
       "      <td>140</td>\n",
       "      <td>[CLS] [VS] 49281041550 51079043620 51079088120...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 3, 8, 4, 2, 7, 6, ...</td>\n",
       "      <td>[0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 0,...</td>\n",
       "      <td>[0, 5309, 5309, 5309, 5309, 5309, 5309, 5309, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.3, 1.78, 1.78, 1.78, 1.84, 1.94...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>12</td>\n",
       "      <td>476</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143478</th>\n",
       "      <td>02cee673-6875-50c9-bad9-e7a7746731eb</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>106</td>\n",
       "      <td>[CLS] [VS] 33332001101 00904224461 00904516561...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52...</td>\n",
       "      <td>[0, 5642, 5642, 5642, 5642, 5642, 5642, 5642, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 2.64, 2.71, 2.71, 4.15, 4.15, 4.4...</td>\n",
       "      <td>[1, 0, 1, 1, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>39</td>\n",
       "      <td>104</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143479 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  patient_id  num_visits  deceased  \\\n",
       "0       35581927-9c95-5ae9-af76-7d74870a349c           1         0   \n",
       "1       f5bba8dd-25c0-5336-8d3d-37424c185026           2         0   \n",
       "2       f4938f91-cadb-5133-8541-a52fb0916cea           2         0   \n",
       "3       6fe2371b-a6f0-5436-aade-7795005b0c66           2         0   \n",
       "4       6f7590ae-f3b9-50e5-9e41-d4bb1000887a           1         0   \n",
       "...                                      ...         ...       ...   \n",
       "143474  3f300d4e-4554-5f1f-9dff-f209a4916cbc           7         0   \n",
       "143475  cf2115d7-937e-511d-b159-dd7eb3d5d420           2         0   \n",
       "143476  31338a39-28f9-54a5-a810-2d05fbaa5166           3         0   \n",
       "143477  0989415d-394c-5f42-8dac-75dc7306a23c           2         0   \n",
       "143478  02cee673-6875-50c9-bad9-e7a7746731eb           2         0   \n",
       "\n",
       "        death_after_start  death_after_end  length  token_length  \\\n",
       "0                     NaN              NaN      50            40   \n",
       "1                     NaN              NaN     148            81   \n",
       "2                     NaN              NaN      78            86   \n",
       "3                     NaN              NaN      86            91   \n",
       "4                     NaN              NaN      72            56   \n",
       "...                   ...              ...     ...           ...   \n",
       "143474                NaN              NaN     536           564   \n",
       "143475                NaN              NaN     166           142   \n",
       "143476                NaN              NaN     283           221   \n",
       "143477                NaN              NaN     470           140   \n",
       "143478                NaN              NaN      98           106   \n",
       "\n",
       "                                        event_tokens_2048  \\\n",
       "0       [CLS] [VS] 00006473900 00904516561 51079000220...   \n",
       "1       [CLS] [VS] 52135_2 52075_2 52074_2 52073_3 520...   \n",
       "2       [CLS] [VS] 0RB30ZZ 0RG10A0 00071101441 0090419...   \n",
       "3       [CLS] [VS] 63739057310 49281041688 00597026010...   \n",
       "4       [CLS] [VS] 50813_0 52135_0 52075_3 52074_3 520...   \n",
       "...                                                   ...   \n",
       "143474  [CLS] [VS] 51484_0 51146_3 51200_1 51221_4 512...   \n",
       "143475  [CLS] [VS] 33332001001 00781305714 10019017644...   \n",
       "143476  [CLS] [VS] 00338011704 00409128331 63323026201...   \n",
       "143477  [CLS] [VS] 49281041550 51079043620 51079088120...   \n",
       "143478  [CLS] [VS] 33332001101 00904224461 00904516561...   \n",
       "\n",
       "                                         type_tokens_2048  \\\n",
       "0       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...   \n",
       "1       [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2       [1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "3       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4       [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "...                                                   ...   \n",
       "143474  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "143475  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 5, ...   \n",
       "143476  [1, 2, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "143477  [1, 2, 6, 6, 6, 6, 6, 6, 6, 3, 8, 4, 2, 7, 6, ...   \n",
       "143478  [1, 2, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "\n",
       "                                          age_tokens_2048  \\\n",
       "0       [0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...   \n",
       "1       [0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...   \n",
       "2       [0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...   \n",
       "3       [0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...   \n",
       "4       [0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...   \n",
       "...                                                   ...   \n",
       "143474  [0, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64...   \n",
       "143475  [0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32...   \n",
       "143476  [0, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50...   \n",
       "143477  [0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 0,...   \n",
       "143478  [0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52...   \n",
       "\n",
       "                                         time_tokens_2048  \\\n",
       "0       [0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...   \n",
       "1       [0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...   \n",
       "2       [0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...   \n",
       "3       [0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...   \n",
       "4       [0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...   \n",
       "...                                                   ...   \n",
       "143474  [0, 6921, 6921, 6921, 6921, 6921, 6921, 6921, ...   \n",
       "143475  [0, 5481, 5481, 5481, 5481, 5481, 5481, 5481, ...   \n",
       "143476  [0, 4997, 4997, 4997, 4997, 4997, 4997, 4997, ...   \n",
       "143477  [0, 5309, 5309, 5309, 5309, 5309, 5309, 5309, ...   \n",
       "143478  [0, 5642, 5642, 5642, 5642, 5642, 5642, 5642, ...   \n",
       "\n",
       "                                        visit_tokens_2048  \\\n",
       "0       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                   ...   \n",
       "143474  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "143475  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "143476  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "143477  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, ...   \n",
       "143478  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                     position_tokens_2048  \\\n",
       "0       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "143474  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "143475  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "143476  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "143477  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...   \n",
       "143478  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      elapsed_tokens_2048  \\\n",
       "0       [-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...   \n",
       "1       [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2       [-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...   \n",
       "3       [-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n",
       "4       [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "...                                                   ...   \n",
       "143474  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "143475  [-2.0, -1.0, 1.16, 1.25, 1.3, 1.3, 1.31, 1.53,...   \n",
       "143476  [-2.0, -1.0, 1.48, 1.49, 1.52, 1.52, 1.6, 1.6,...   \n",
       "143477  [-2.0, -1.0, 1.3, 1.78, 1.78, 1.78, 1.84, 1.94...   \n",
       "143478  [-2.0, -1.0, 2.64, 2.71, 2.71, 4.15, 4.15, 4.4...   \n",
       "\n",
       "                     common_conditions                 rare_conditions  \\\n",
       "0       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                ...                             ...   \n",
       "143474  [1, 1, 1, 1, 0, 0, 0, 0, 1, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "143475  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "143476  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "143477  [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "143478  [1, 0, 1, 1, 0, 1, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "        last_VS_index  last_VE_index  length_of_stay  \\\n",
       "0                   1             52        2.041667   \n",
       "1                  68            154        3.500000   \n",
       "2                  49             84        1.375000   \n",
       "3                  55             92        1.458333   \n",
       "4                   1             74        2.958333   \n",
       "...               ...            ...             ...   \n",
       "143474            520            562        1.666667   \n",
       "143475            109            172        2.541667   \n",
       "143476            192            293        4.125000   \n",
       "143477             12            476       19.250000   \n",
       "143478             39            104        2.625000   \n",
       "\n",
       "        label_length_of_stay_1week  \n",
       "0                            False  \n",
       "1                            False  \n",
       "2                            False  \n",
       "3                            False  \n",
       "4                            False  \n",
       "...                            ...  \n",
       "143474                       False  \n",
       "143475                       False  \n",
       "143476                       False  \n",
       "143477                        True  \n",
       "143478                       False  \n",
       "\n",
       "[143479 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_length_of_stay_dataset(dataset: pd.DataFrame, threshold: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"Process the length of stay dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        threshold (int): The threshold length of stay.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['last_VE_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VE]'))\n",
    "    dataset['length_of_stay'] = dataset.apply(get_length_of_stay, axis=1)\n",
    "\n",
    "    dataset = filter_by_length_of_stay(dataset, threshold=1)\n",
    "    dataset['label_length_of_stay_1week'] = dataset['length_of_stay'] >= threshold\n",
    "    dataset['event_tokens_2048'] = dataset.apply(lambda row: truncate_visit_after_threshold(row, threshold=24), axis=1)\n",
    "\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_los = process_length_of_stay_dataset(dataset_2048.copy(), threshold=7)\n",
    "dataset_2048_los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_condition_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the condition dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input condition dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed condition dataset.\n",
    "    \"\"\"\n",
    "    dataset['all_conditions'] = dataset.apply(\n",
    "        lambda row: np.concatenate(\n",
    "            [row['common_conditions'], row['rare_conditions']], dtype=np.float64), axis=1\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for conditions including rare and common\n",
    "dataset_2048_condition = process_condition_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:16.075719400Z",
     "start_time": "2024-03-13T16:15:12.335721100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mortality_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the mortality dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input mortality dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed mortality dataset.\n",
    "    \"\"\"\n",
    "    dataset['label_mortality_2weeks'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 15)).astype(int)\n",
    "    dataset['label_mortality_1month'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 32)).astype(int)\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:47.326996100Z",
     "start_time": "2024-03-13T16:15:16.094719300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_readmission_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the readmission dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['label_readmission_1month'] = dataset.apply(check_readmission_label, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset.apply(remove_last_visit, axis=1)\n",
    "    dataset['num_visits'] -= 1\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = filter_by_num_visit(dataset_2048.copy(), minimum_num_visits=2)\n",
    "dataset_2048_readmission = process_readmission_dataset(dataset_2048_readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(dataset: pd.DataFrame, target: str, test_size: float, return_test: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Split the given dataset into training and testing sets using iterative stratification on given multi-label target.\n",
    "    \"\"\"\n",
    "    # Convert all_conditions into a format suitable for multi-label stratification\n",
    "    Y = np.array(dataset[target].values.tolist())\n",
    "    X = dataset['patient_id'].to_numpy().reshape(-1, 1)\n",
    "    is_single_label = type(dataset.iloc[0][target]) == np.int64\n",
    "\n",
    "    # Perform stratified split\n",
    "    if is_single_label:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = iterative_train_test_split(X, Y, test_size=test_size)\n",
    "    \n",
    "    X_train = X_train.flatten().tolist()\n",
    "    X_test = X_test.flatten().tolist()\n",
    "\n",
    "    if return_test:\n",
    "        return X_test\n",
    "    else:\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "def sample_balanced_subset(dataset: pd.DataFrame, target: str, sample_size: int):\n",
    "    \"\"\"\n",
    "    Sample a subset of dataset with balanced target labels.\n",
    "    \"\"\"\n",
    "    # Sampling positive and negative patients\n",
    "    pos_patients = dataset[dataset[target] == True].sample(n=sample_size // 2, random_state=SEED)\n",
    "    neg_patients = dataset[dataset[target] == False].sample(n=sample_size // 2, random_state=SEED)\n",
    "\n",
    "    # Combining and shuffling patient IDs\n",
    "    sample_patients = pos_patients['patient_id'].tolist() + neg_patients['patient_id'].tolist()\n",
    "    random.shuffle(sample_patients)\n",
    "\n",
    "    return sample_patients\n",
    "\n",
    "\n",
    "def get_pretrain_test_split(dataset: pd.DataFrame, stratify_target: Optional[str] = None, test_size: float = 0.15):\n",
    "    \"\"\" Split dataset into pretrain and test set. Stratify on a given target column if needed. \"\"\"\n",
    "\n",
    "    if stratify_target:\n",
    "        pretrain_ids, test_ids = stratified_train_test_split(dataset, target=stratify_target, test_size=test_size)\n",
    "    \n",
    "    else:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=SEED)\n",
    "        test_ids = test_patients['patient_id'].tolist()\n",
    "        pretrain_ids = dataset[~dataset['patient_id'].isin(test_patients)]['patient_id'].tolist()\n",
    "    \n",
    "    random.shuffle(pretrain_ids)\n",
    "\n",
    "    return pretrain_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "patient_ids_dict = {'pretrain': [], 'finetune': {'few_shot': {}, 'kfold': {}}, 'test': []}\n",
    "\n",
    "# Get train-test split\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(dataset_2048_readmission, stratify_target='label_readmission_1month', test_size=0.2)\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(process_condition_dataset, stratify_target='all_conditions', test_size=0.15)\n",
    "# patient_ids_dict['pretrain'] = pretrain_ids\n",
    "# patient_ids_dict['test'] = test_ids\n",
    "\n",
    "pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "patient_ids_dict['pretrain'] = pid['pretrain']\n",
    "patient_ids_dict['test'] = pid['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "\n",
    "    task_splits = {\n",
    "\n",
    "        # 'mortality': {\n",
    "        #     'dataset': dataset_2048_mortality,\n",
    "        #     'label_col': 'label_mortality_1month',\n",
    "        #     'finetune_size': [250, 500, 1000, 5000, 20000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_mortality.pkl',\n",
    "        #     'split_mode': 'single_label_balanced'\n",
    "        # },\n",
    "\n",
    "        # 'readmission': {\n",
    "        #     'dataset': dataset_2048_readmission,\n",
    "        #     'label_col': 'label_readmission_1month',\n",
    "        #     'finetune_size': [250, 1000, 5000, 20000, 60000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_readmission.pkl',\n",
    "        #     'split_mode': 'single_label_stratified'\n",
    "        # },\n",
    "\n",
    "        'length_of_stay': {\n",
    "            'dataset': dataset_2048_los,\n",
    "            'label_col': 'label_length_of_stay_1week',\n",
    "            'finetune_size': [250, 1000, 5000, 20000, 50000],\n",
    "            'save_path': 'patient_id_dict/dataset_2048_los.pkl',\n",
    "            'split_mode': 'single_label_balanced'\n",
    "        },\n",
    "\n",
    "        # 'condition': {\n",
    "        #     'dataset': dataset_2048_condition,\n",
    "        #     'label_col': 'all_conditions',\n",
    "        #     'finetune_size': [50000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_condition.pkl',\n",
    "        #     'split_mode': 'multi_label_stratified'\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    all_tasks = list(task_splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:51.800996200Z",
     "start_time": "2024-03-13T16:15:50.494996100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to disk: patient_id_dict/dataset_2048_los.pkl\n"
     ]
    }
   ],
   "source": [
    "def get_finetune_split(\n",
    "        config: config,\n",
    "        patient_ids_dict: Dict[str, Any],\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-finetuneation sets using k-fold cross-finetuneation\n",
    "    while ensuring balanced label distribution in each fold. Saves the resulting dictionary to disk.\n",
    "    \"\"\"\n",
    "    # Extract task-specific configuration\n",
    "    task_config = config.task_splits[task]\n",
    "    dataset = task_config['dataset']\n",
    "    label_col = task_config['label_col']\n",
    "    finetune_sizes = task_config['finetune_size']\n",
    "    save_path = task_config['save_path']\n",
    "    split_mode = task_config['split_mode']\n",
    "\n",
    "    # Get pretrain dataset\n",
    "    pretrain_ids = patient_ids_dict['pretrain']\n",
    "    dataset = dataset[dataset['patient_id'].isin(pretrain_ids)]\n",
    "\n",
    "    # Few-shot finetune patient ids\n",
    "    for finetune_num in finetune_sizes:\n",
    "\n",
    "        if split_mode == 'single_label_balanced':\n",
    "            finetune_ids = sample_balanced_subset(dataset, target=label_col, sample_size=finetune_num)\n",
    "        \n",
    "        elif split_mode == 'single_label_stratified':\n",
    "            finetune_ids = stratified_train_test_split(dataset, target=label_col, test_size=finetune_num / len(dataset), return_test=True)\n",
    "        \n",
    "        elif split_mode == 'multi_label_stratified':\n",
    "            finetune_ids = stratified_train_test_split(dataset, target=label_col, test_size=finetune_num / len(dataset), return_test=True)\n",
    "\n",
    "        patient_ids_dict['finetune']['few_shot'][f'{finetune_num}'] = finetune_ids\n",
    "    \n",
    "    # Save the dictionary to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n",
    "        print(f'File saved to disk: {save_path}')\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "\n",
    "for task in config.all_tasks:\n",
    "    patient_ids_dict = get_finetune_split(\n",
    "        config=config,\n",
    "        patient_ids_dict=patient_ids_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:14:10.181184300Z",
     "start_time": "2024-03-13T14:13:39.154567400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_mortality.to_parquet('patient_sequences/patient_sequences_2048_mortality.parquet')\n",
    "# dataset_2048_readmission.to_parquet('patient_sequences/patient_sequences_2048_readmission.parquet')\n",
    "dataset_2048_los.to_parquet('patient_sequences/patient_sequences_2048_los.parquet')\n",
    "# dataset_2048_condition.to_parquet('patient_sequences/patient_sequences_2048_condition.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2048_condition = pd.read_parquet('patient_sequences/patient_sequences_2048_condition.parquet')\n",
    "pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "condition_finetune = dataset_2048_condition.loc[dataset_2048_condition['patient_id'].isin(pid['finetune']['few_shot']['50000'])]\n",
    "condition_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.array(condition_finetune['all_conditions'].tolist()).sum(axis=0)\n",
    "weights = np.clip(0, 50, sum(freq) / freq)\n",
    "np.max(np.sqrt(freq)) / np.sqrt(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(patient_ids_dict['pretrain']) == sorted(pickle.load(open('new_data/patient_id_dict/sample_pretrain_test_patient_ids_with_conditions.pkl', 'rb'))['pretrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(dataset_2048_mortality, dataset_2048_readmission, how='outer', on='patient_id')\n",
    "# final_merged_df = pd.merge(merged_df, dataset_2048_condition, how='outer', on='patient_id')\n",
    "# final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing stratified k-fold split\n",
    "    # skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "    #     dataset_cv = dataset.iloc[cv_index]\n",
    "    #     dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "    #     # Separate positive and negative labeled patients\n",
    "    #     pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "    #     neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "    #     # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "    #     num_pos_needed = cv_size // 2\n",
    "    #     num_neg_needed = cv_size // 2\n",
    "\n",
    "    #     # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "    #     cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "    #     remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "    #     # Extract patient IDs for training set\n",
    "    #     finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "    #     finetune_patients += remaining_finetune_patients\n",
    "\n",
    "    #     # Shuffle each list of patients\n",
    "    #     random.shuffle(cv_patients)\n",
    "    #     random.shuffle(finetune_patients)\n",
    "\n",
    "    #     patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "plt.ylim(0, 6000)\n",
    "plt.xlabel('Length of Event Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Event Tokens Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
