{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:14:45.546088300Z",
     "start_time": "2024-03-13T16:14:43.587090300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "from os.path import join\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "DATASET = 'patient_sequences/patient_sequences_2048.parquet'\n",
    "MAX_LEN = 2048\n",
    "\n",
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:12.321718600Z",
     "start_time": "2024-03-13T16:14:45.553089800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_2048</th>\n",
       "      <th>type_tokens_2048</th>\n",
       "      <th>age_tokens_2048</th>\n",
       "      <th>time_tokens_2048</th>\n",
       "      <th>visit_tokens_2048</th>\n",
       "      <th>position_tokens_2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9b62c9f4-3fdc-5020-82b5-ae5b8292445a</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>55</td>\n",
       "      <td>[[CLS], [VS], 7569, 66689036430, 00904224461, ...</td>\n",
       "      <td>[1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 8, ...</td>\n",
       "      <td>[0, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28...</td>\n",
       "      <td>[0, 5963, 5963, 5963, 5963, 5963, 5963, 5963, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ca522eb-dd89-5f79-8155-9599ea46b0b2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>244.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>51</td>\n",
       "      <td>55</td>\n",
       "      <td>[[CLS], [VS], 00904629261, 00904642281, 009046...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86...</td>\n",
       "      <td>[0, 8016, 8016, 8016, 8016, 8016, 8016, 8016, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54ee4964-056b-5c38-a607-b95e63176fc3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1810</td>\n",
       "      <td>1882</td>\n",
       "      <td>[[CLS], [VS], 63323026201, 00603385521, 005970...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71...</td>\n",
       "      <td>[0, 5520, 5520, 5520, 5520, 5520, 5520, 5520, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02adf8a6-8bc0-55d3-81ae-4d8582094896</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>640</td>\n",
       "      <td>672</td>\n",
       "      <td>[[CLS], [VS], 51079045420, 00006494300, 177140...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, ...</td>\n",
       "      <td>[0, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65...</td>\n",
       "      <td>[0, 8002, 8002, 8002, 8002, 8002, 8002, 8002, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>744fe3c4-9b03-55ae-ac9f-6bc4e967cde7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>[[CLS], [VS], 7813, 7813, 7902, 7902, 9604, 00...</td>\n",
       "      <td>[1, 2, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...</td>\n",
       "      <td>[0, 7582, 7582, 7582, 7582, 7582, 7582, 7582, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173666</th>\n",
       "      <td>cf2115d7-937e-511d-b159-dd7eb3d5d420</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>174</td>\n",
       "      <td>[[CLS], [VS], 66591018442, 63323026201, 001350...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32...</td>\n",
       "      <td>[0, 5481, 5481, 5481, 5481, 5481, 5481, 5481, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173667</th>\n",
       "      <td>31338a39-28f9-54a5-a810-2d05fbaa5166</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283</td>\n",
       "      <td>295</td>\n",
       "      <td>[[CLS], [VS], 5014, 5123, 00338011704, 6332302...</td>\n",
       "      <td>[1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50...</td>\n",
       "      <td>[0, 4997, 4997, 4997, 4997, 4997, 4997, 4997, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173668</th>\n",
       "      <td>0989415d-394c-5f42-8dac-75dc7306a23c</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470</td>\n",
       "      <td>478</td>\n",
       "      <td>[[CLS], [VS], 51079043620, 51079088120, 492810...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 3, 8, 4, 2, 7, 7, ...</td>\n",
       "      <td>[0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 0,...</td>\n",
       "      <td>[0, 5309, 5309, 5309, 5309, 5309, 5309, 5309, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173669</th>\n",
       "      <td>26fb8fef-b976-5c55-859d-cc190261f94b</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>[[CLS], [VS], 0SRD0J9, 60505251903, 0090422446...</td>\n",
       "      <td>[1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61...</td>\n",
       "      <td>[0, 5085, 5085, 5085, 5085, 5085, 5085, 5085, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173670</th>\n",
       "      <td>02cee673-6875-50c9-bad9-e7a7746731eb</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>106</td>\n",
       "      <td>[[CLS], [VS], 49502069724, 00409490234, 003380...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52...</td>\n",
       "      <td>[0, 5642, 5642, 5642, 5642, 5642, 5642, 5642, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169576 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  patient_id  num_visits  deceased  \\\n",
       "1       9b62c9f4-3fdc-5020-82b5-ae5b8292445a           3         0   \n",
       "2       2ca522eb-dd89-5f79-8155-9599ea46b0b2           1         1   \n",
       "3       54ee4964-056b-5c38-a607-b95e63176fc3          18         1   \n",
       "4       02adf8a6-8bc0-55d3-81ae-4d8582094896           8         1   \n",
       "5       744fe3c4-9b03-55ae-ac9f-6bc4e967cde7           2         0   \n",
       "...                                      ...         ...       ...   \n",
       "173666  cf2115d7-937e-511d-b159-dd7eb3d5d420           2         0   \n",
       "173667  31338a39-28f9-54a5-a810-2d05fbaa5166           3         0   \n",
       "173668  0989415d-394c-5f42-8dac-75dc7306a23c           2         0   \n",
       "173669  26fb8fef-b976-5c55-859d-cc190261f94b           2         0   \n",
       "173670  02cee673-6875-50c9-bad9-e7a7746731eb           2         0   \n",
       "\n",
       "        death_after_start  death_after_end  length  token_length  \\\n",
       "1                     NaN              NaN      43            55   \n",
       "2                   244.0            242.0      51            55   \n",
       "3                    13.0              0.0    1810          1882   \n",
       "4                    20.0             11.0     640           672   \n",
       "5                     NaN              NaN      80            88   \n",
       "...                   ...              ...     ...           ...   \n",
       "173666                NaN              NaN     166           174   \n",
       "173667                NaN              NaN     283           295   \n",
       "173668                NaN              NaN     470           478   \n",
       "173669                NaN              NaN      93           101   \n",
       "173670                NaN              NaN      98           106   \n",
       "\n",
       "                                        event_tokens_2048  \\\n",
       "1       [[CLS], [VS], 7569, 66689036430, 00904224461, ...   \n",
       "2       [[CLS], [VS], 00904629261, 00904642281, 009046...   \n",
       "3       [[CLS], [VS], 63323026201, 00603385521, 005970...   \n",
       "4       [[CLS], [VS], 51079045420, 00006494300, 177140...   \n",
       "5       [[CLS], [VS], 7813, 7813, 7902, 7902, 9604, 00...   \n",
       "...                                                   ...   \n",
       "173666  [[CLS], [VS], 66591018442, 63323026201, 001350...   \n",
       "173667  [[CLS], [VS], 5014, 5123, 00338011704, 6332302...   \n",
       "173668  [[CLS], [VS], 51079043620, 51079088120, 492810...   \n",
       "173669  [[CLS], [VS], 0SRD0J9, 60505251903, 0090422446...   \n",
       "173670  [[CLS], [VS], 49502069724, 00409490234, 003380...   \n",
       "\n",
       "                                         type_tokens_2048  \\\n",
       "1       [1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 8, ...   \n",
       "2       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "3       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4       [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, ...   \n",
       "5       [1, 2, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "...                                                   ...   \n",
       "173666  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "173667  [1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "173668  [1, 2, 6, 6, 6, 6, 6, 6, 6, 3, 8, 4, 2, 7, 7, ...   \n",
       "173669  [1, 2, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "173670  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "\n",
       "                                          age_tokens_2048  \\\n",
       "1       [0, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28...   \n",
       "2       [0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86...   \n",
       "3       [0, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71...   \n",
       "4       [0, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65...   \n",
       "5       [0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...   \n",
       "...                                                   ...   \n",
       "173666  [0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32...   \n",
       "173667  [0, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50...   \n",
       "173668  [0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 0,...   \n",
       "173669  [0, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61...   \n",
       "173670  [0, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52...   \n",
       "\n",
       "                                         time_tokens_2048  \\\n",
       "1       [0, 5963, 5963, 5963, 5963, 5963, 5963, 5963, ...   \n",
       "2       [0, 8016, 8016, 8016, 8016, 8016, 8016, 8016, ...   \n",
       "3       [0, 5520, 5520, 5520, 5520, 5520, 5520, 5520, ...   \n",
       "4       [0, 8002, 8002, 8002, 8002, 8002, 8002, 8002, ...   \n",
       "5       [0, 7582, 7582, 7582, 7582, 7582, 7582, 7582, ...   \n",
       "...                                                   ...   \n",
       "173666  [0, 5481, 5481, 5481, 5481, 5481, 5481, 5481, ...   \n",
       "173667  [0, 4997, 4997, 4997, 4997, 4997, 4997, 4997, ...   \n",
       "173668  [0, 5309, 5309, 5309, 5309, 5309, 5309, 5309, ...   \n",
       "173669  [0, 5085, 5085, 5085, 5085, 5085, 5085, 5085, ...   \n",
       "173670  [0, 5642, 5642, 5642, 5642, 5642, 5642, 5642, ...   \n",
       "\n",
       "                                        visit_tokens_2048  \\\n",
       "1       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "5       [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                   ...   \n",
       "173666  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "173667  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "173668  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, ...   \n",
       "173669  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "173670  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                     position_tokens_2048  \n",
       "1       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                   ...  \n",
       "173666  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "173667  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "173668  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, ...  \n",
       "173669  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "173670  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[169576 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "# dataset_2048.drop(\n",
    "#     ['event_tokens', 'type_tokens', 'age_tokens', 'time_tokens', 'visit_tokens', 'position_tokens'],\n",
    "#     axis=1,\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "dataset_2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:16.075719400Z",
     "start_time": "2024-03-13T16:15:12.335721100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mortality_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the mortality dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input mortality dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed mortality dataset.\n",
    "    \"\"\"\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    dataset['label_mortality_2weeks'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 15)).astype(int)\n",
    "    dataset['label_mortality_1month'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 32)).astype(int)\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:47.326996100Z",
     "start_time": "2024-03-13T16:15:16.094719300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_readmission_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the readmission dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['label_readmission_1month'] = dataset.apply(check_readmission_label, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset.apply(remove_last_visit, axis=1)\n",
    "    dataset['num_visits'] -= 1\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)\n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    return dataset\n",
    "\n",
    "def filter_by_num_visit(dataset: pd.DataFrame, minimum_num_visits: int) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on num_visits threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold num_visits\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['num_visits'] >= minimum_num_visits].copy()\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "def get_last_index(seq: List[str], target: str) -> int:\n",
    "    \"\"\"Return the index of the last occurrence of target in seq.\n",
    "\n",
    "    Args:\n",
    "        seq (List[str]): The input sequence.\n",
    "        target (str): The target string to find.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the last occurrence of target in seq.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_last_index(['A', 'B', 'A', 'A', 'C', 'D'], 'A')\n",
    "        3\n",
    "    \"\"\"\n",
    "    return len(seq) - (seq[::-1].index(target) + 1)\n",
    "\n",
    "def truncate_and_pad(row: pd.Series) -> Any:\n",
    "    \"\"\"Return a truncated and padded version of row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        Any: The truncated and padded row.\n",
    "\n",
    "    Note:\n",
    "        This function assumes the presence of the following columns in row:\n",
    "        - 'event_tokens_2048'\n",
    "        - 'type_tokens_2048'\n",
    "        - 'age_tokens_2048'\n",
    "        - 'time_tokens_2048'\n",
    "        - 'visit_tokens_2048'\n",
    "        - 'position_tokens_2048'\n",
    "    \"\"\"\n",
    "    seq_len = len(row['event_tokens_2048'])\n",
    "    row['type_tokens_2048'] = np.pad(row['type_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['age_tokens_2048'] = np.pad(row['age_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['time_tokens_2048'] = np.pad(row['time_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['visit_tokens_2048'] = np.pad(row['visit_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['position_tokens_2048'] = np.pad(row['position_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    return row\n",
    "\n",
    "def check_readmission_label(row: pd.Series) -> int:\n",
    "    \"\"\"Check if the label indicates readmission within one month.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if readmission label is present, False otherwise.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return int(row['event_tokens_2048'][last_vs_index - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'))\n",
    "\n",
    "def remove_last_visit(row: pd.Series) -> pd.Series:\n",
    "    \"\"\" Remove the event tokens of last visit in the row\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return row['event_tokens_2048'][:last_vs_index - 1]\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = filter_by_num_visit(dataset_2048, minimum_num_visits=2)\n",
    "dataset_2048_readmission = process_readmission_dataset(dataset_2048_readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:51.800996200Z",
     "start_time": "2024-03-13T16:15:50.494996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_dataset_train_test_finetune_datasets(\n",
    "        dataset: pd.DataFrame,\n",
    "        label_col: str,\n",
    "        cv_size: int,\n",
    "        test_size: int,\n",
    "        finetune_size: List[int],\n",
    "        num_splits: int,\n",
    "        save_path: str\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-finetuneation sets using k-fold cross-finetuneation\n",
    "    while ensuring balanced label distribution in each fold. Saves the resulting dictionary to disk.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        label_col (str): The name of the column containing the labels.\n",
    "        cv_size (int): The number of patients in each cross-finetuneation split.\n",
    "        test_size (int): The number of patients in the test set.\n",
    "        finetune_size (List[int]): The number of patients in each fine-tune set\n",
    "        num_splits (int): The number of splits to create (k value).\n",
    "        save_path (str): The path to save the resulting dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, List[str]]]: A dictionary containing patient IDs for each split group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to hold patient IDs for different sets\n",
    "    patient_ids_dict = {'pretrain': [], 'finetune': {'few_shot': {}, 'kfold': {}}, 'test': []}\n",
    "\n",
    "    # Sample test patients and remove them from dataset\n",
    "    if test_size > 0:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=23)\n",
    "        dataset.drop(test_patients.index, inplace=True)\n",
    "        patient_ids_dict['test'] = test_patients['patient_id'].tolist()\n",
    "\n",
    "    # Any remaining data is used for pretraining\n",
    "    patient_ids_dict['pretrain'] = dataset['patient_id'].tolist()\n",
    "    random.shuffle(patient_ids_dict['pretrain'])\n",
    "\n",
    "    # few_shot finetune dataset\n",
    "    for each_finetune_size in finetune_size:\n",
    "        subset_size = each_finetune_size // 2\n",
    "\n",
    "        # Sampling positive and negative patients\n",
    "        pos_patients = dataset[dataset[label_col] == True].sample(n=subset_size, random_state=23)\n",
    "        neg_patients = dataset[dataset[label_col] == False].sample(n=subset_size, random_state=23)\n",
    "\n",
    "        # Extracting patient IDs\n",
    "        pos_patients_ids = pos_patients['patient_id'].tolist()\n",
    "        neg_patients_ids = neg_patients['patient_id'].tolist()\n",
    "\n",
    "        # Combining and shuffling patient IDs\n",
    "        finetune_patients = pos_patients_ids + neg_patients_ids\n",
    "        random.shuffle(finetune_patients)\n",
    "        patient_ids_dict['finetune']['few_shot'][f'{each_finetune_size}'] = finetune_patients\n",
    "\n",
    "    # Performing stratified k-fold split\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=23)\n",
    "\n",
    "    for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "        dataset_cv = dataset.iloc[cv_index]\n",
    "        dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "        # Separate positive and negative labeled patients\n",
    "        pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "        neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "        # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "        num_pos_needed = cv_size // 2\n",
    "        num_neg_needed = cv_size // 2\n",
    "\n",
    "        # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "        cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "        remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "        # Extract patient IDs for training set\n",
    "        finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "        finetune_patients += remaining_finetune_patients\n",
    "\n",
    "        # Shuffle each list of patients\n",
    "        random.shuffle(cv_patients)\n",
    "        random.shuffle(finetune_patients)\n",
    "\n",
    "        patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}\n",
    "\n",
    "    # Save the dictionary to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "\n",
    "patient_ids_dict = split_dataset_train_test_finetune_datasets(\n",
    "    dataset=dataset_2048_mortality.copy(),\n",
    "    label_col='label_mortality_1month',\n",
    "    cv_size=4000,\n",
    "    test_size=20000,    # 20000\n",
    "    finetune_size=[250, 500, 1000, 5000, 20000],  # [250, 500, 1000, 5000, 20000] [250, 1000, 5000, 20000, 50000]\n",
    "    num_splits=5,\n",
    "    save_path='patient_id_dict/dataset_2048_mortality_1month.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:59.448997Z",
     "start_time": "2024-03-13T16:15:59.326996300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['250', '500', '1000', '5000', '20000'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('patient_id_dict/dataset_2048_mortality_1month.pkl', 'rb'))['finetune']['few_shot'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:14:10.181184300Z",
     "start_time": "2024-03-13T14:13:39.154567400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_mortality.to_parquet('patient_sequences_2048_mortality.parquet')\n",
    "dataset_2048_readmission.to_parquet('patient_sequences/patient_sequences_2048_readmission.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "plt.ylim(0, 6000)\n",
    "plt.xlabel('Length of Event Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Event Tokens Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
