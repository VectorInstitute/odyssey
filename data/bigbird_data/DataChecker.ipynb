{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:14:45.546088300Z",
     "start_time": "2024-03-13T16:14:43.587090300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from os.path import join\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "DATA_ROOT = '/h/afallah/odyssey/odyssey/data/bigbird_data'\n",
    "DATASET = f'{DATA_ROOT}/patient_sequences/patient_sequences_2048.parquet'\n",
    "MAX_LEN = 2048\n",
    "\n",
    "SEED = 23\n",
    "os.chdir(DATA_ROOT)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:12.321718600Z",
     "start_time": "2024-03-13T16:14:45.553089800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "# dataset_2048.drop(\n",
    "#     ['event_tokens', 'type_tokens', 'age_tokens', 'time_tokens', 'visit_tokens', 'position_tokens'],\n",
    "#     axis=1,\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "print(f'Current columns: {dataset_2048.columns}')\n",
    "dataset_2048.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_and_pad(row: pd.Series) -> Any:\n",
    "    \"\"\"Return a truncated and padded version of row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        Any: The truncated and padded row.\n",
    "\n",
    "    Note:\n",
    "        This function assumes the presence of the following columns in row:\n",
    "        - 'event_tokens_2048'\n",
    "        - 'type_tokens_2048'\n",
    "        - 'age_tokens_2048'\n",
    "        - 'time_tokens_2048'\n",
    "        - 'visit_tokens_2048'\n",
    "        - 'position_tokens_2048'\n",
    "        - 'elapsed_tokens_2048'\n",
    "    \"\"\"\n",
    "    seq_len = len(row['event_tokens_2048'])\n",
    "    row['type_tokens_2048'] = np.pad(row['type_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['age_tokens_2048'] = np.pad(row['age_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['time_tokens_2048'] = np.pad(row['time_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['visit_tokens_2048'] = np.pad(row['visit_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['position_tokens_2048'] = np.pad(row['position_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['elapsed_tokens_2048'] = np.pad(row['elapsed_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    return row\n",
    "\n",
    "\n",
    "def filter_by_num_visit(dataset: pd.DataFrame, minimum_num_visits: int) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on num_visits threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold num_visits\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['num_visits'] >= minimum_num_visits]\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def filter_by_length_of_stay(dataset: pd.DataFrame, threshold: int = 1) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on length of stay threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold length of stay\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['length_of_stay'] >= threshold]\n",
    "\n",
    "    # Only keep the patients that their first event happens within threshold\n",
    "    filtered_dataset = filtered_dataset[\n",
    "        filtered_dataset.apply(\n",
    "        lambda row: row['elapsed_tokens_2048'][row['last_VS_index'] + 1] < threshold*24,\n",
    "        axis=1)]\n",
    "\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def get_last_index(seq: List[str], target: str) -> int:\n",
    "    \"\"\"Return the index of the last occurrence of target in seq.\n",
    "\n",
    "    Args:\n",
    "        seq (List[str]): The input sequence.\n",
    "        target (str): The target string to find.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the last occurrence of target in seq.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_last_index(['A', 'B', 'A', 'A', 'C', 'D'], 'A')\n",
    "        3\n",
    "    \"\"\"\n",
    "    return len(seq) - (seq[::-1].index(target) + 1)\n",
    "\n",
    "\n",
    "def check_readmission_label(row: pd.Series) -> int:\n",
    "    \"\"\"Check if the label indicates readmission within one month.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if readmission label is present, False otherwise.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return int(row['event_tokens_2048'][last_vs_index - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'))\n",
    "\n",
    "\n",
    "def get_length_of_stay(row: pd.Series) -> pd.Series:\n",
    "    \"\"\" Determine the length of a given visit. \n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.    \n",
    "    \"\"\"\n",
    "    admission_time = row['last_VS_index'] + 1\n",
    "    discharge_time = row['last_VE_index'] - 1\n",
    "    return (discharge_time - admission_time) / 24\n",
    "\n",
    "\n",
    "def get_visit_cutoff_at_threshold(row: pd.Series, threshold: int = 24) -> int:\n",
    "    \"\"\" Get the index of the first event token of last visit that occurs after threshold hours.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "        threshold (int): The number of hours to consider.\n",
    "\n",
    "    Returns:\n",
    "        cutoff_index (int): The corrosponding cutoff index.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    last_ve_index = row['last_VE_index']\n",
    "\n",
    "    for i in range(last_vs_index+1, last_ve_index):\n",
    "        if row['elapsed_tokens_2048'][i] > threshold:\n",
    "            return i\n",
    "    \n",
    "    return len(row['event_tokens_2048'])\n",
    "\n",
    "\n",
    "def join_token_list_to_sring(df: pd.DataFrame, column_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Joins lists of tokens into single strings for a specified column in a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        - df: pandas.DataFrame. The DataFrame containing the column to be modified.\n",
    "        - column_name: str. The name of the column containing lists of tokens to be joined.\n",
    "    \n",
    "    Returns:\n",
    "        None. The DataFrame is modified in place.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = df[column_name].transform(lambda token_list: ' '.join(token_list))\n",
    "    else:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_length_of_stay_dataset(dataset: pd.DataFrame, threshold: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"Process the length of stay dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        threshold (int): The threshold length of stay.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['last_VE_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VE]'))\n",
    "    dataset['length_of_stay'] = dataset.apply(get_length_of_stay, axis=1)\n",
    "\n",
    "    dataset = filter_by_length_of_stay(dataset, threshold=1)\n",
    "    dataset['label_los_1week'] = (dataset['length_of_stay'] >= threshold).astype(int)\n",
    "    \n",
    "    dataset['los_cutoff'] = dataset.apply(lambda row: get_visit_cutoff_at_threshold(row, threshold=24), axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset.apply(lambda row: row['event_tokens_2048'][:row['los_cutoff']], axis=1)\n",
    "\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    join_token_list_to_sring(dataset, 'event_tokens_2048')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for length of stay prediction above a threshold\n",
    "dataset_2048_los = process_length_of_stay_dataset(dataset_2048.copy(), threshold=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_condition_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the condition dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input condition dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed condition dataset.\n",
    "    \"\"\"\n",
    "    dataset['all_conditions'] = dataset.apply(\n",
    "        lambda row: np.concatenate(\n",
    "            [row['common_conditions'], row['rare_conditions']], dtype=np.int64), axis=1\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    join_token_list_to_sring(dataset, 'event_tokens_2048')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for conditions including rare and common\n",
    "dataset_2048_condition = process_condition_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:16.075719400Z",
     "start_time": "2024-03-13T16:15:12.335721100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mortality_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the mortality dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input mortality dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed mortality dataset.\n",
    "    \"\"\"\n",
    "    dataset['label_mortality_2weeks'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 15)).astype(int)\n",
    "    dataset['label_mortality_1month'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 32)).astype(int)\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    join_token_list_to_sring(dataset, 'event_tokens_2048')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:47.326996100Z",
     "start_time": "2024-03-13T16:15:16.094719300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_readmission_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the readmission dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['readmission_cutoff'] = dataset['last_VS_index'] - 1\n",
    "    dataset['label_readmission_1month'] = dataset.apply(check_readmission_label, axis=1)\n",
    "    \n",
    "    dataset['event_tokens_2048'] = dataset.apply(lambda row: row['event_tokens_2048'][:row['readmission_cutoff']], axis=1)\n",
    "    dataset['num_visits'] -= 1\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)\n",
    "    \n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    join_token_list_to_sring(dataset, 'event_tokens_2048')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = filter_by_num_visit(dataset_2048.copy(), minimum_num_visits=2)\n",
    "dataset_2048_readmission = process_readmission_dataset(dataset_2048_readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_dataset(datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Process the multi-task dataset by merging the original dataset with the other datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (Dict): Dictionary mapping each task to its respective dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed multi-task dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Merging datasets on 'patient_id'\n",
    "    merged_dataset = datasets['original'].merge(datasets['condition'][['patient_id', 'all_conditions']], on='patient_id', how='left')\n",
    "    merged_dataset = merged_dataset.merge(datasets['mortality'][['patient_id', 'label_mortality_1month']], on='patient_id', how='left')\n",
    "    merged_dataset = merged_dataset.merge(datasets['readmission'][['patient_id', 'readmission_cutoff', 'label_readmission_1month']], on='patient_id', how='left')\n",
    "    merged_dataset = merged_dataset.merge(datasets['los'][['patient_id', 'los_cutoff', 'label_los_1week']], on='patient_id', how='left')\n",
    "\n",
    "    # Selecting the required columns\n",
    "    merged_dataset = merged_dataset[['patient_id', 'num_visits', 'event_tokens_2048', 'type_tokens_2048', 'age_tokens_2048', \n",
    "                                    'time_tokens_2048', 'visit_tokens_2048', 'position_tokens_2048', 'elapsed_tokens_2048',\n",
    "                                    'los_cutoff', 'readmission_cutoff', 'all_conditions',\n",
    "                                    'label_mortality_1month', 'label_readmission_1month', 'label_los_1week']]\n",
    "\n",
    "    # Transform conditions from a vector of numbers to binary classes\n",
    "    conditions_expanded = merged_dataset['all_conditions'].apply(pd.Series)\n",
    "    conditions_expanded.columns = [f'condition{i}' for i in range(20)]\n",
    "    merged_dataset = merged_dataset.drop('all_conditions', axis=1)\n",
    "    merged_dataset = pd.concat([merged_dataset, conditions_expanded], axis=1)\n",
    "\n",
    "    # join_token_list_to_sring(merged_dataset, 'event_tokens_2048')\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "multi_dataset = process_multi_dataset(\n",
    "    datasets={'original': dataset_2048, 'mortality': dataset_2048_mortality, 'condition': dataset_2048_condition,\n",
    "        'readmission': dataset_2048_readmission, 'los': dataset_2048_los}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id                               3638fa69-2779-57e6-b398-ab4f4cfb709c\n",
       "num_visits                                                                  4\n",
       "event_tokens_2048           [[CLS], [VS], 00338004304, 63323026201, 001211...\n",
       "type_tokens_2048            [1, 2, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, ...\n",
       "age_tokens_2048             [0, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25...\n",
       "time_tokens_2048            [0, 5572, 5572, 5572, 5572, 5572, 5572, 5572, ...\n",
       "visit_tokens_2048           [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...\n",
       "position_tokens_2048        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "elapsed_tokens_2048         [-2.0, -1.0, 0.54, 4.12, 4.12, 4.12, 4.12, 10....\n",
       "los_cutoff                                                              268.0\n",
       "readmission_cutoff                                                      244.0\n",
       "label_mortality_1month                                                      0\n",
       "label_readmission_1month                                                  0.0\n",
       "label_los_1week                                                           0.0\n",
       "condition0                                                                  0\n",
       "condition1                                                                  0\n",
       "condition2                                                                  0\n",
       "condition3                                                                  1\n",
       "condition4                                                                  0\n",
       "condition5                                                                  0\n",
       "condition6                                                                  0\n",
       "condition7                                                                  0\n",
       "condition8                                                                  0\n",
       "condition9                                                                  0\n",
       "condition10                                                                 0\n",
       "condition11                                                                 0\n",
       "condition12                                                                 0\n",
       "condition13                                                                 0\n",
       "condition14                                                                 0\n",
       "condition15                                                                 0\n",
       "condition16                                                                 0\n",
       "condition17                                                                 0\n",
       "condition18                                                                 0\n",
       "condition19                                                                 0\n",
       "Name: 10000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_dataset.iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(dataset: pd.DataFrame, target: str, test_size: float, return_test: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Split the given dataset into training and testing sets using iterative stratification on given multi-label target.\n",
    "    \"\"\"\n",
    "    # Convert all_conditions into a format suitable for multi-label stratification\n",
    "    Y = np.array(dataset[target].values.tolist())\n",
    "    X = dataset['patient_id'].to_numpy().reshape(-1, 1)\n",
    "    is_single_label = type(dataset.iloc[0][target]) == np.int64\n",
    "\n",
    "    # Perform stratified split\n",
    "    if is_single_label:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = iterative_train_test_split(X, Y, test_size=test_size)\n",
    "    \n",
    "    X_train = X_train.flatten().tolist()\n",
    "    X_test = X_test.flatten().tolist()\n",
    "\n",
    "    if return_test:\n",
    "        return X_test\n",
    "    else:\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "def sample_balanced_subset(dataset: pd.DataFrame, target: str, sample_size: int):\n",
    "    \"\"\"\n",
    "    Sample a subset of dataset with balanced target labels.\n",
    "    \"\"\"\n",
    "    # Sampling positive and negative patients\n",
    "    pos_patients = dataset[dataset[target] == True].sample(n=sample_size // 2, random_state=SEED)\n",
    "    neg_patients = dataset[dataset[target] == False].sample(n=sample_size // 2, random_state=SEED)\n",
    "\n",
    "    # Combining and shuffling patient IDs\n",
    "    sample_patients = pos_patients['patient_id'].tolist() + neg_patients['patient_id'].tolist()\n",
    "    random.shuffle(sample_patients)\n",
    "\n",
    "    return sample_patients\n",
    "\n",
    "\n",
    "def get_pretrain_test_split(dataset: pd.DataFrame, stratify_target: Optional[str] = None, test_size: float = 0.15):\n",
    "    \"\"\" Split dataset into pretrain and test set. Stratify on a given target column if needed. \"\"\"\n",
    "\n",
    "    if stratify_target:\n",
    "        pretrain_ids, test_ids = stratified_train_test_split(dataset, target=stratify_target, test_size=test_size)\n",
    "    \n",
    "    else:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=SEED)\n",
    "        test_ids = test_patients['patient_id'].tolist()\n",
    "        pretrain_ids = dataset[~dataset['patient_id'].isin(test_patients)]['patient_id'].tolist()\n",
    "    \n",
    "    random.shuffle(pretrain_ids)\n",
    "\n",
    "    return pretrain_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "patient_ids_dict = {'pretrain': [], 'finetune': {'few_shot': {}, 'kfold': {}}, 'test': []}\n",
    "\n",
    "# Get train-test split\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(dataset_2048_readmission, stratify_target='label_readmission_1month', test_size=0.2)\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(process_condition_dataset, stratify_target='all_conditions', test_size=0.15)\n",
    "# patient_ids_dict['pretrain'] = pretrain_ids\n",
    "# patient_ids_dict['test'] = test_ids\n",
    "\n",
    "pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "patient_ids_dict['pretrain'] = pid['pretrain']\n",
    "patient_ids_dict['test'] = pid['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "\n",
    "    task_splits = {\n",
    "\n",
    "        # 'mortality': {\n",
    "        #     'dataset': dataset_2048_mortality,\n",
    "        #     'label_col': 'label_mortality_1month',\n",
    "        #     'finetune_size': [250, 500, 1000, 5000, 20000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_mortality.pkl',\n",
    "        #     'split_mode': 'single_label_balanced'\n",
    "        # },\n",
    "\n",
    "        # 'readmission': {\n",
    "        #     'dataset': dataset_2048_readmission,\n",
    "        #     'label_col': 'label_readmission_1month',\n",
    "        #     'finetune_size': [250, 1000, 5000, 20000, 60000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_readmission.pkl',\n",
    "        #     'split_mode': 'single_label_stratified'\n",
    "        # },\n",
    "\n",
    "        'length_of_stay': {\n",
    "            'dataset': dataset_2048_los,\n",
    "            'label_col': 'label_los_1week',\n",
    "            'finetune_size': [250, 1000, 5000, 20000, 50000],\n",
    "            'save_path': 'patient_id_dict/dataset_2048_los.pkl',\n",
    "            'split_mode': 'single_label_balanced'\n",
    "        },\n",
    "\n",
    "        # 'condition': {\n",
    "        #     'dataset': dataset_2048_condition,\n",
    "        #     'label_col': 'all_conditions',\n",
    "        #     'finetune_size': [50000],\n",
    "        #     'save_path': 'patient_id_dict/dataset_2048_condition.pkl',\n",
    "        #     'split_mode': 'multi_label_stratified'\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    all_tasks = list(task_splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:51.800996200Z",
     "start_time": "2024-03-13T16:15:50.494996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_finetune_split(\n",
    "        config: config,\n",
    "        patient_ids_dict: Dict[str, Any],\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-finetuneation sets using k-fold cross-finetuneation\n",
    "    while ensuring balanced label distribution in each fold. Saves the resulting dictionary to disk.\n",
    "    \"\"\"\n",
    "    # Extract task-specific configuration\n",
    "    task_config = config.task_splits[task]\n",
    "    dataset = task_config['dataset']\n",
    "    label_col = task_config['label_col']\n",
    "    finetune_sizes = task_config['finetune_size']\n",
    "    save_path = task_config['save_path']\n",
    "    split_mode = task_config['split_mode']\n",
    "\n",
    "    # Get pretrain dataset\n",
    "    pretrain_ids = patient_ids_dict['pretrain']\n",
    "    dataset = dataset[dataset['patient_id'].isin(pretrain_ids)]\n",
    "\n",
    "    # Few-shot finetune patient ids\n",
    "    for finetune_num in finetune_sizes:\n",
    "\n",
    "        if split_mode == 'single_label_balanced':\n",
    "            finetune_ids = sample_balanced_subset(dataset, target=label_col, sample_size=finetune_num)\n",
    "        \n",
    "        elif split_mode == 'single_label_stratified':\n",
    "            finetune_ids = stratified_train_test_split(dataset, target=label_col, test_size=finetune_num / len(dataset), return_test=True)\n",
    "        \n",
    "        elif split_mode == 'multi_label_stratified':\n",
    "            finetune_ids = stratified_train_test_split(dataset, target=label_col, test_size=finetune_num / len(dataset), return_test=True)\n",
    "\n",
    "        patient_ids_dict['finetune']['few_shot'][f'{finetune_num}'] = finetune_ids\n",
    "    \n",
    "    # Save the dictionary to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n",
    "        print(f'File saved to disk: {save_path}')\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "\n",
    "for task in config.all_tasks:\n",
    "    patient_ids_dict = get_finetune_split(\n",
    "        config=config,\n",
    "        patient_ids_dict=patient_ids_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:14:10.181184300Z",
     "start_time": "2024-03-13T14:13:39.154567400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_mortality.to_parquet('patient_sequences/patient_sequences_2048_mortality.parquet')\n",
    "# dataset_2048_readmission.to_parquet('patient_sequences/patient_sequences_2048_readmission.parquet')\n",
    "dataset_2048_los.to_parquet('patient_sequences/patient_sequences_2048_los.parquet')\n",
    "# dataset_2048_condition.to_parquet('patient_sequences/patient_sequences_2048_condition.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2048_condition = pd.read_parquet('patient_sequences/patient_sequences_2048_condition.parquet')\n",
    "pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "condition_finetune = dataset_2048_condition.loc[dataset_2048_condition['patient_id'].isin(pid['finetune']['few_shot']['50000'])]\n",
    "condition_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.array(condition_finetune['all_conditions'].tolist()).sum(axis=0)\n",
    "weights = np.clip(0, 50, sum(freq) / freq)\n",
    "np.max(np.sqrt(freq)) / np.sqrt(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(patient_ids_dict['pretrain']) == sorted(pickle.load(open('new_data/patient_id_dict/sample_pretrain_test_patient_ids_with_conditions.pkl', 'rb'))['pretrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(dataset_2048_mortality, dataset_2048_readmission, how='outer', on='patient_id')\n",
    "# final_merged_df = pd.merge(merged_df, dataset_2048_condition, how='outer', on='patient_id')\n",
    "# final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing stratified k-fold split\n",
    "    # skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "    #     dataset_cv = dataset.iloc[cv_index]\n",
    "    #     dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "    #     # Separate positive and negative labeled patients\n",
    "    #     pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "    #     neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "    #     # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "    #     num_pos_needed = cv_size // 2\n",
    "    #     num_neg_needed = cv_size // 2\n",
    "\n",
    "    #     # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "    #     cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "    #     remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "    #     # Extract patient IDs for training set\n",
    "    #     finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "    #     finetune_patients += remaining_finetune_patients\n",
    "\n",
    "    #     # Shuffle each list of patients\n",
    "    #     random.shuffle(cv_patients)\n",
    "    #     random.shuffle(finetune_patients)\n",
    "\n",
    "    #     patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "plt.ylim(0, 6000)\n",
    "plt.xlabel('Length of Event Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Event Tokens Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
