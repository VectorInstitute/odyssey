{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:14:45.546088300Z",
     "start_time": "2024-03-13T16:14:43.587090300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from os.path import join\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "DATA_ROOT = '/h/afallah/odyssey/odyssey/data/bigbird_data'\n",
    "DATASET = f'{DATA_ROOT}/patient_sequences/patient_sequences_2048.parquet'\n",
    "MAX_LEN = 2048\n",
    "\n",
    "os.chdir(DATA_ROOT)\n",
    "random.seed(23)\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:12.321718600Z",
     "start_time": "2024-03-13T16:14:45.553089800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_visits</th>\n",
       "      <th>deceased</th>\n",
       "      <th>death_after_start</th>\n",
       "      <th>death_after_end</th>\n",
       "      <th>length</th>\n",
       "      <th>token_length</th>\n",
       "      <th>event_tokens_2048</th>\n",
       "      <th>type_tokens_2048</th>\n",
       "      <th>age_tokens_2048</th>\n",
       "      <th>time_tokens_2048</th>\n",
       "      <th>visit_tokens_2048</th>\n",
       "      <th>position_tokens_2048</th>\n",
       "      <th>elapsed_tokens_2048</th>\n",
       "      <th>common_conditions</th>\n",
       "      <th>rare_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35581927-9c95-5ae9-af76-7d74870a349c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>[[CLS], [VS], 00006473900, 00904516561, 510790...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...</td>\n",
       "      <td>[0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...</td>\n",
       "      <td>[0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f5bba8dd-25c0-5336-8d3d-37424c185026</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>156</td>\n",
       "      <td>[[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...</td>\n",
       "      <td>[0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4938f91-cadb-5133-8541-a52fb0916cea</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>[[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...</td>\n",
       "      <td>[1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...</td>\n",
       "      <td>[0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe2371b-a6f0-5436-aade-7795005b0c66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>[[CLS], [VS], 63739057310, 49281041688, 005970...</td>\n",
       "      <td>[1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>[0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...</td>\n",
       "      <td>[0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6f7590ae-f3b9-50e5-9e41-d4bb1000887a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>[[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...</td>\n",
       "      <td>[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...</td>\n",
       "      <td>[0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...</td>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             patient_id  num_visits  deceased  \\\n",
       "0  35581927-9c95-5ae9-af76-7d74870a349c           1         0   \n",
       "1  f5bba8dd-25c0-5336-8d3d-37424c185026           2         0   \n",
       "2  f4938f91-cadb-5133-8541-a52fb0916cea           2         0   \n",
       "3  6fe2371b-a6f0-5436-aade-7795005b0c66           2         0   \n",
       "4  6f7590ae-f3b9-50e5-9e41-d4bb1000887a           1         0   \n",
       "\n",
       "   death_after_start  death_after_end  length  token_length  \\\n",
       "0                NaN              NaN      50            54   \n",
       "1                NaN              NaN     148           156   \n",
       "2                NaN              NaN      78            86   \n",
       "3                NaN              NaN      86            94   \n",
       "4                NaN              NaN      72            76   \n",
       "\n",
       "                                   event_tokens_2048  \\\n",
       "0  [[CLS], [VS], 00006473900, 00904516561, 510790...   \n",
       "1  [[CLS], [VS], 52135_2, 52075_2, 52074_2, 52073...   \n",
       "2  [[CLS], [VS], 0RB30ZZ, 0RG10A0, 00071101441, 0...   \n",
       "3  [[CLS], [VS], 63739057310, 49281041688, 005970...   \n",
       "4  [[CLS], [VS], 50813_0, 52135_0, 52075_3, 52074...   \n",
       "\n",
       "                                    type_tokens_2048  \\\n",
       "0  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, ...   \n",
       "1  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "2  [1, 2, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "3  [1, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...   \n",
       "4  [1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "\n",
       "                                     age_tokens_2048  \\\n",
       "0  [0, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85...   \n",
       "1  [0, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83...   \n",
       "2  [0, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44...   \n",
       "3  [0, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72...   \n",
       "4  [0, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47...   \n",
       "\n",
       "                                    time_tokens_2048  \\\n",
       "0  [0, 5902, 5902, 5902, 5902, 5902, 5902, 5902, ...   \n",
       "1  [0, 6594, 6594, 6594, 6594, 6594, 6594, 6594, ...   \n",
       "2  [0, 8150, 8150, 8150, 8150, 8150, 8150, 8150, ...   \n",
       "3  [0, 6093, 6093, 6093, 6093, 6093, 6093, 6093, ...   \n",
       "4  [0, 6379, 6379, 6379, 6379, 6379, 6379, 6379, ...   \n",
       "\n",
       "                                   visit_tokens_2048  \\\n",
       "0  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4  [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                position_tokens_2048  \\\n",
       "0  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                 elapsed_tokens_2048  \\\n",
       "0  [-2.0, -1.0, 1.97, 2.02, 2.02, 2.02, 2.02, 2.0...   \n",
       "1  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  [-2.0, -1.0, 0.0, 0.0, 1.08, 1.08, 13.89, 13.8...   \n",
       "3  [-2.0, -1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n",
       "4  [-2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                common_conditions                 rare_conditions  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "# dataset_2048.drop(\n",
    "#     ['event_tokens', 'type_tokens', 'age_tokens', 'time_tokens', 'visit_tokens', 'position_tokens'],\n",
    "#     axis=1,\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "dataset_2048.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_condition_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the condition dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input condition dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed condition dataset.\n",
    "    \"\"\"\n",
    "    dataset_2048['all_conditions'] = dataset_2048.apply(\n",
    "        lambda row: np.concatenate(\n",
    "            [row['common_conditions'], row['rare_conditions']], dtype=np.float64), axis=1\n",
    "    )\n",
    "    \n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(\n",
    "        lambda token_list: ' '.join(token_list)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for conditions including rare and common\n",
    "dataset_2048 = process_condition_dataset(dataset_2048)\n",
    "dataset_2048.to_parquet(f'{DATA_ROOT}/patient_sequences/patient_sequences_2048_with_conditions.parquet')\n",
    "dataset_2048.iloc[:100].to_parquet(f'{DATA_ROOT}/patient_sequences/patient_sequences_2048_with_conditions_100patients.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_data_split(dataset: pd.DataFrame, target: str, test_size: float = 0.15):\n",
    "    \"\"\"\n",
    "    Split the given dataset into training and testing sets using iterative stratification on given multi-label target.\n",
    "    \"\"\"\n",
    "    # Convert all_conditions into a format suitable for multi-label stratification\n",
    "    Y = np.array(dataset_2048[target].values.tolist())\n",
    "\n",
    "    # We will split based on Y but we need to keep the association with patient_id\n",
    "    X = dataset_2048['patient_id'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # Perform iterative stratification\n",
    "    X_train, y_train, X_test, y_test = iterative_train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    return X_train.flatten().tolist(), X_test.flatten().tolist()\n",
    "\n",
    "pretrain_ids, test_ids = get_stratified_data_split(dataset_2048, 'all_conditions', test_size=0.15)\n",
    "\n",
    "patient_ids_dict = {'pretrain': pretrain_ids, 'finetune': {'few_shot': {}, 'kfold': {}}, 'test': test_ids}\n",
    "\n",
    "with open('sample_pretrain_test_patient_ids_with_conditions.pkl', 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:16.075719400Z",
     "start_time": "2024-03-13T16:15:12.335721100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_mortality_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the mortality dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input mortality dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed mortality dataset.\n",
    "    \"\"\"\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    dataset['label_mortality_2weeks'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 15)).astype(int)\n",
    "    dataset['label_mortality_1month'] = ((dataset['death_after_start'] >= 0) & (dataset['death_after_end'] <= 32)).astype(int)\n",
    "    return dataset\n",
    "\n",
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:47.326996100Z",
     "start_time": "2024-03-13T16:15:16.094719300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_readmission_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process the readmission dataset to extract required features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset.\n",
    "    \"\"\"\n",
    "    dataset.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "    dataset['last_VS_index'] = dataset['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "    dataset['label_readmission_1month'] = dataset.apply(check_readmission_label, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset.apply(remove_last_visit, axis=1)\n",
    "    dataset['num_visits'] -= 1\n",
    "    dataset['token_length'] = dataset['event_tokens_2048'].apply(len)\n",
    "    dataset = dataset.apply(truncate_and_pad, axis=1)\n",
    "    dataset['event_tokens_2048'] = dataset['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "    return dataset\n",
    "\n",
    "def filter_by_num_visit(dataset: pd.DataFrame, minimum_num_visits: int) -> pd.DataFrame:\n",
    "    \"\"\" Filter the patients based on num_visits threshold.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        minimum_num_visits (int): The threshold num_visits\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset.loc[dataset['num_visits'] >= minimum_num_visits].copy()\n",
    "    filtered_dataset.reset_index(drop=True, inplace=True)\n",
    "    return filtered_dataset\n",
    "\n",
    "def get_last_index(seq: List[str], target: str) -> int:\n",
    "    \"\"\"Return the index of the last occurrence of target in seq.\n",
    "\n",
    "    Args:\n",
    "        seq (List[str]): The input sequence.\n",
    "        target (str): The target string to find.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the last occurrence of target in seq.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_last_index(['A', 'B', 'A', 'A', 'C', 'D'], 'A')\n",
    "        3\n",
    "    \"\"\"\n",
    "    return len(seq) - (seq[::-1].index(target) + 1)\n",
    "\n",
    "def truncate_and_pad(row: pd.Series) -> Any:\n",
    "    \"\"\"Return a truncated and padded version of row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        Any: The truncated and padded row.\n",
    "\n",
    "    Note:\n",
    "        This function assumes the presence of the following columns in row:\n",
    "        - 'event_tokens_2048'\n",
    "        - 'type_tokens_2048'\n",
    "        - 'age_tokens_2048'\n",
    "        - 'time_tokens_2048'\n",
    "        - 'visit_tokens_2048'\n",
    "        - 'position_tokens_2048'\n",
    "    \"\"\"\n",
    "    seq_len = len(row['event_tokens_2048'])\n",
    "    row['type_tokens_2048'] = np.pad(row['type_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['age_tokens_2048'] = np.pad(row['age_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['time_tokens_2048'] = np.pad(row['time_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['visit_tokens_2048'] = np.pad(row['visit_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    row['position_tokens_2048'] = np.pad(row['position_tokens_2048'][:seq_len], (0, MAX_LEN - seq_len), mode='constant')\n",
    "    return row\n",
    "\n",
    "def check_readmission_label(row: pd.Series) -> int:\n",
    "    \"\"\"Check if the label indicates readmission within one month.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if readmission label is present, False otherwise.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return int(row['event_tokens_2048'][last_vs_index - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'))\n",
    "\n",
    "def remove_last_visit(row: pd.Series) -> pd.Series:\n",
    "    \"\"\" Remove the event tokens of last visit in the row\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The input row.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The preprocessed row.\n",
    "    \"\"\"\n",
    "    last_vs_index = row['last_VS_index']\n",
    "    return row['event_tokens_2048'][:last_vs_index - 1]\n",
    "\n",
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = filter_by_num_visit(dataset_2048, minimum_num_visits=2)\n",
    "dataset_2048_readmission = process_readmission_dataset(dataset_2048_readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:15:51.800996200Z",
     "start_time": "2024-03-13T16:15:50.494996100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_dataset_train_test_finetune_datasets(\n",
    "        dataset: pd.DataFrame,\n",
    "        label_col: str,\n",
    "        cv_size: int,\n",
    "        test_size: int,\n",
    "        finetune_size: List[int],\n",
    "        num_splits: int,\n",
    "        save_path: str\n",
    ") -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-finetuneation sets using k-fold cross-finetuneation\n",
    "    while ensuring balanced label distribution in each fold. Saves the resulting dictionary to disk.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        label_col (str): The name of the column containing the labels.\n",
    "        cv_size (int): The number of patients in each cross-finetuneation split.\n",
    "        test_size (int): The number of patients in the test set.\n",
    "        finetune_size (List[int]): The number of patients in each fine-tune set\n",
    "        num_splits (int): The number of splits to create (k value).\n",
    "        save_path (str): The path to save the resulting dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, List[str]]]: A dictionary containing patient IDs for each split group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to hold patient IDs for different sets\n",
    "    patient_ids_dict = {'pretrain': [], 'finetune': {'few_shot': {}, 'kfold': {}}, 'test': []}\n",
    "\n",
    "    # Sample test patients and remove them from dataset\n",
    "    if test_size > 0:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=23)\n",
    "        dataset.drop(test_patients.index, inplace=True)\n",
    "        patient_ids_dict['test'] = test_patients['patient_id'].tolist()\n",
    "\n",
    "    # Any remaining data is used for pretraining\n",
    "    patient_ids_dict['pretrain'] = dataset['patient_id'].tolist()\n",
    "    random.shuffle(patient_ids_dict['pretrain'])\n",
    "\n",
    "    # few_shot finetune dataset\n",
    "    for each_finetune_size in finetune_size:\n",
    "\n",
    "        # For the time being\n",
    "        test_patients = dataset.sample(n=each_finetune_size, random_state=23)\n",
    "        test_patients = test_patients['patient_id'].tolist()\n",
    "        random.shuffle(test_patients)\n",
    "        patient_ids_dict['finetune']['few_shot'][f'{each_finetune_size}'] = test_patients\n",
    "\n",
    "        subset_size = each_finetune_size // 2\n",
    "\n",
    "        # Sampling positive and negative patients\n",
    "        pos_patients = dataset[dataset[label_col] == True].sample(n=subset_size, random_state=23)\n",
    "        neg_patients = dataset[dataset[label_col] == False].sample(n=subset_size, random_state=23)\n",
    "\n",
    "        # Extracting patient IDs\n",
    "        pos_patients_ids = pos_patients['patient_id'].tolist()\n",
    "        neg_patients_ids = neg_patients['patient_id'].tolist()\n",
    "\n",
    "        # Combining and shuffling patient IDs\n",
    "        finetune_patients = pos_patients_ids + neg_patients_ids\n",
    "        random.shuffle(finetune_patients)\n",
    "        patient_ids_dict['finetune']['few_shot'][f'{each_finetune_size}'] = finetune_patients\n",
    "\n",
    "    # Performing stratified k-fold split\n",
    "    # skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=23)\n",
    "\n",
    "    # for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "    #     dataset_cv = dataset.iloc[cv_index]\n",
    "    #     dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "    #     # Separate positive and negative labeled patients\n",
    "    #     pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "    #     neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "    #     # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "    #     num_pos_needed = cv_size // 2\n",
    "    #     num_neg_needed = cv_size // 2\n",
    "\n",
    "    #     # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "    #     cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "    #     remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "    #     # Extract patient IDs for training set\n",
    "    #     finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "    #     finetune_patients += remaining_finetune_patients\n",
    "\n",
    "    #     # Shuffle each list of patients\n",
    "    #     random.shuffle(cv_patients)\n",
    "    #     random.shuffle(finetune_patients)\n",
    "\n",
    "    #     patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}\n",
    "\n",
    "    # Save the dictionary to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "patient_ids_dict = split_dataset_train_test_finetune_datasets(\n",
    "    dataset=dataset_2048_condition.copy(),\n",
    "    label_col='all_conditions',\n",
    "    cv_size=4000,\n",
    "    test_size=20000,    # for mortality used to be 20000, then 15000, then 20000 | readmission 10000 then, now 8000\n",
    "    finetune_size=[250, 1000, 5000, 20000, 50000],  # [250, 500, 1000, 5000, 20000] [250, 1000, 5000, 20000, 50000]\n",
    "    num_splits=5,\n",
    "    save_path='patient_id_dict/dataset_2048_condition.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:14:10.181184300Z",
     "start_time": "2024-03-13T14:13:39.154567400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_mortality.to_parquet('patient_sequences/patient_sequences_2048_mortality.parquet')\n",
    "# dataset_2048_readmission.to_parquet('patient_sequences/patient_sequences_2048_readmission.parquet')\n",
    "dataset_2048_condition.to_parquet('patient_sequences/patient_sequences_2048_condition.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "plt.ylim(0, 6000)\n",
    "plt.xlabel('Length of Event Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Event Tokens Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
